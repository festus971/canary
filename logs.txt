
==> Audit <==
|-----------|--------------------------------|--------------|----------|---------|---------------------|---------------------|
|  Command  |              Args              |   Profile    |   User   | Version |     Start Time      |      End Time       |
|-----------|--------------------------------|--------------|----------|---------|---------------------|---------------------|
| delete    | -p doubledocker                | doubledocker | fkiprop1 | v1.33.1 | 18 Sep 24 14:22 EAT | 18 Sep 24 14:22 EAT |
| delete    |                                | minikube     | fkiprop1 | v1.33.1 | 18 Sep 24 14:23 EAT | 18 Sep 24 14:23 EAT |
| node      | list                           | minikube     | fkiprop1 | v1.33.1 | 18 Sep 24 14:26 EAT |                     |
| start     |                                | minikube     | fkiprop1 | v1.33.1 | 19 Sep 24 10:13 EAT |                     |
| start     |                                | minikube     | fkiprop1 | v1.33.1 | 19 Sep 24 10:14 EAT |                     |
| start     |                                | minikube     | fkiprop1 | v1.33.1 | 19 Sep 24 10:15 EAT | 19 Sep 24 10:15 EAT |
| kubectl   | -- get namespaces              | minikube     | fkiprop1 | v1.33.1 | 19 Sep 24 10:37 EAT | 19 Sep 24 10:38 EAT |
| kubectl   | -- get nodes                   | minikube     | fkiprop1 | v1.33.1 | 19 Sep 24 10:38 EAT | 19 Sep 24 10:38 EAT |
| kubectl   | -- get pods                    | minikube     | fkiprop1 | v1.33.1 | 19 Sep 24 10:39 EAT | 19 Sep 24 10:39 EAT |
| addons    | list                           | minikube     | fkiprop1 | v1.33.1 | 19 Sep 24 10:46 EAT | 19 Sep 24 10:46 EAT |
| addons    | enable metrics-server          | minikube     | fkiprop1 | v1.33.1 | 19 Sep 24 10:47 EAT | 19 Sep 24 10:47 EAT |
| addons    | enable dashboard               | minikube     | fkiprop1 | v1.33.1 | 19 Sep 24 10:47 EAT | 19 Sep 24 10:47 EAT |
| addons    | list                           | minikube     | fkiprop1 | v1.33.1 | 19 Sep 24 10:47 EAT | 19 Sep 24 10:47 EAT |
| dashboard |                                | minikube     | fkiprop1 | v1.33.1 | 19 Sep 24 10:47 EAT |                     |
| dashboard |                                | minikube     | fkiprop1 | v1.33.1 | 19 Sep 24 10:53 EAT |                     |
| dashboard | --url                          | minikube     | fkiprop1 | v1.33.1 | 19 Sep 24 10:53 EAT |                     |
| dashboard |                                | minikube     | fkiprop1 | v1.33.1 | 19 Sep 24 11:35 EAT |                     |
| dashboard |                                | minikube     | fkiprop1 | v1.33.1 | 19 Sep 24 11:47 EAT |                     |
| delete    | minipod                        | minikube     | fkiprop1 | v1.33.1 | 19 Sep 24 12:14 EAT |                     |
| stop      |                                | minikube     | fkiprop1 | v1.33.1 | 19 Sep 24 12:15 EAT | 19 Sep 24 12:15 EAT |
| delete    |                                | minikube     | fkiprop1 | v1.33.1 | 19 Sep 24 12:15 EAT | 19 Sep 24 12:15 EAT |
| start     |                                | minikube     | fkiprop1 | v1.33.1 | 19 Sep 24 12:19 EAT | 19 Sep 24 12:19 EAT |
| dashboard |                                | minikube     | fkiprop1 | v1.33.1 | 19 Sep 24 12:20 EAT |                     |
| addons    | list                           | minikube     | fkiprop1 | v1.33.1 | 19 Sep 24 12:25 EAT | 19 Sep 24 12:25 EAT |
| addons    | enable dashboard               | minikube     | fkiprop1 | v1.33.1 | 19 Sep 24 12:26 EAT | 19 Sep 24 12:26 EAT |
| addons    | enable mertics-server          | minikube     | fkiprop1 | v1.33.1 | 19 Sep 24 12:26 EAT |                     |
| addons    | enable metrics-server          | minikube     | fkiprop1 | v1.33.1 | 19 Sep 24 12:26 EAT | 19 Sep 24 12:26 EAT |
| addons    | list                           | minikube     | fkiprop1 | v1.33.1 | 19 Sep 24 12:27 EAT | 19 Sep 24 12:27 EAT |
| dashboard |                                | minikube     | fkiprop1 | v1.33.1 | 19 Sep 24 12:27 EAT |                     |
| start     | --driver=virtualbox            | multivbox    | fkiprop1 | v1.33.1 | 19 Sep 24 12:34 EAT |                     |
|           | --nodes=3 --disk-size=10g      |              |          |         |                     |                     |
|           | --cpus=2 --memory=6g           |              |          |         |                     |                     |
|           | --kubernetes-version=v1.27.12  |              |          |         |                     |                     |
|           | --cni=calico                   |              |          |         |                     |                     |
|           | --container-runtime=cri-o -p   |              |          |         |                     |                     |
|           | multivbox                      |              |          |         |                     |                     |
| start     | --driver=virtualbox            | multivbox    | fkiprop1 | v1.33.1 | 19 Sep 24 12:40 EAT |                     |
|           | --nodes=3 --disk-size=10g      |              |          |         |                     |                     |
|           | --cpus=2 --memory=6g           |              |          |         |                     |                     |
|           | --kubernetes-version=v1.27.12  |              |          |         |                     |                     |
|           | --cni=calico                   |              |          |         |                     |                     |
|           | --container-runtime=cri-o -p   |              |          |         |                     |                     |
|           | multivbox                      |              |          |         |                     |                     |
| start     | --driver=docker                | multivbox    | fkiprop1 | v1.33.1 | 19 Sep 24 12:44 EAT | 19 Sep 24 12:46 EAT |
|           | --nodes=3 --disk-size=10g      |              |          |         |                     |                     |
|           | --cpus=2 --memory=6g           |              |          |         |                     |                     |
|           | --kubernetes-version=v1.27.12  |              |          |         |                     |                     |
|           | --cni=calico                   |              |          |         |                     |                     |
|           | --container-runtime=cri-o -p   |              |          |         |                     |                     |
|           | multivbox                      |              |          |         |                     |                     |
| dashboard |                                | minikube     | fkiprop1 | v1.33.1 | 19 Sep 24 12:52 EAT |                     |
| dashboard |                                | minikube     | fkiprop1 | v1.33.1 | 19 Sep 24 14:26 EAT |                     |
| dashboard |                                | minikube     | fkiprop1 | v1.33.1 | 19 Sep 24 14:34 EAT |                     |
| start     |                                | minikube     | fkiprop1 | v1.33.1 | 20 Sep 24 10:28 EAT | 20 Sep 24 10:28 EAT |
| addons    | enable metrics-server          | minikube     | fkiprop1 | v1.33.1 | 20 Sep 24 10:29 EAT | 20 Sep 24 10:29 EAT |
| addons    | enable dashboard               | minikube     | fkiprop1 | v1.33.1 | 20 Sep 24 10:29 EAT | 20 Sep 24 10:29 EAT |
| dashboard |                                | minikube     | fkiprop1 | v1.33.1 | 20 Sep 24 10:30 EAT |                     |
| stop      |                                | minikube     | fkiprop1 | v1.33.1 | 20 Sep 24 10:38 EAT | 20 Sep 24 10:38 EAT |
| start     |                                | minikube     | fkiprop1 | v1.33.1 | 20 Sep 24 10:39 EAT | 20 Sep 24 10:39 EAT |
| dashboard |                                | minikube     | fkiprop1 | v1.33.1 | 20 Sep 24 10:39 EAT |                     |
| start     |                                | minikube     | fkiprop1 | v1.33.1 | 28 Sep 24 10:33 EAT | 28 Sep 24 10:33 EAT |
| config    | view                           | minikube     | fkiprop1 | v1.33.1 | 30 Sep 24 09:51 EAT | 30 Sep 24 09:51 EAT |
| dashboard |                                | minikube     | fkiprop1 | v1.33.1 | 30 Sep 24 09:54 EAT |                     |
| stop      |                                | minikube     | fkiprop1 | v1.33.1 | 30 Sep 24 09:55 EAT | 30 Sep 24 09:55 EAT |
| start     |                                | minikube     | fkiprop1 | v1.33.1 | 07 Nov 24 17:10 EAT | 07 Nov 24 17:10 EAT |
| stop      |                                | minikube     | fkiprop1 | v1.33.1 | 07 Nov 24 17:11 EAT | 07 Nov 24 17:12 EAT |
| start     |                                | minikube     | fkiprop1 | v1.33.1 | 09 Nov 24 22:29 EAT | 09 Nov 24 22:30 EAT |
| addons    | enable ingress                 | minikube     | fkiprop1 | v1.33.1 | 09 Nov 24 22:30 EAT |                     |
| addons    | enable ingress                 | minikube     | fkiprop1 | v1.33.1 | 09 Nov 24 22:36 EAT |                     |
| stop      |                                | minikube     | fkiprop1 | v1.33.1 | 09 Nov 24 22:44 EAT | 09 Nov 24 22:44 EAT |
| start     | --cpus=4 --memory=4096         | minikube     | fkiprop1 | v1.33.1 | 09 Nov 24 22:44 EAT | 09 Nov 24 22:44 EAT |
| addons    | enable ingress                 | minikube     | fkiprop1 | v1.33.1 | 09 Nov 24 22:45 EAT |                     |
| stop      |                                | minikube     | fkiprop1 | v1.33.1 | 10 Nov 24 09:08 EAT | 10 Nov 24 09:08 EAT |
| start     | --cpus=4 --memory=8192         | minikube     | fkiprop1 | v1.33.1 | 10 Nov 24 09:09 EAT |                     |
| start     | --cpus=4 --memory=8192         | minikube     | fkiprop1 | v1.33.1 | 10 Nov 24 09:15 EAT |                     |
| start     |                                | minikube     | fkiprop1 | v1.33.1 | 10 Nov 24 09:16 EAT | 10 Nov 24 09:17 EAT |
| addons    | enable ingress                 | minikube     | fkiprop1 | v1.33.1 | 10 Nov 24 09:17 EAT |                     |
| tunnel    |                                | minikube     | fkiprop1 | v1.33.1 | 10 Nov 24 09:23 EAT | 10 Nov 24 09:40 EAT |
|-----------|--------------------------------|--------------|----------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/11/10 09:16:09
Running on machine: LTTECHMAC799-2
Binary: Built with gc go1.22.3 for darwin/arm64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1110 09:16:09.857141    7073 out.go:291] Setting OutFile to fd 1 ...
I1110 09:16:09.857859    7073 out.go:343] isatty.IsTerminal(1) = true
I1110 09:16:09.857861    7073 out.go:304] Setting ErrFile to fd 2...
I1110 09:16:09.857864    7073 out.go:343] isatty.IsTerminal(2) = true
I1110 09:16:09.858207    7073 root.go:338] Updating PATH: /Users/fkiprop1/.minikube/bin
I1110 09:16:09.861963    7073 out.go:298] Setting JSON to false
I1110 09:16:09.894680    7073 start.go:129] hostinfo: {"hostname":"LTTECHMAC799-2.local","uptime":561469,"bootTime":1730657900,"procs":604,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"15.0.1","kernelVersion":"24.0.0","kernelArch":"arm64","virtualizationSystem":"","virtualizationRole":"","hostId":"095991dc-ecf2-5722-b7f9-b6409845032a"}
W1110 09:16:09.894757    7073 start.go:137] gopshost.Virtualization returned error: not implemented yet
I1110 09:16:09.899808    7073 out.go:177] üòÑ  minikube v1.33.1 on Darwin 15.0.1 (arm64)
I1110 09:16:09.906625    7073 notify.go:220] Checking for updates...
I1110 09:16:09.906762    7073 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I1110 09:16:09.907920    7073 driver.go:392] Setting default libvirt URI to qemu:///system
I1110 09:16:10.040202    7073 docker.go:122] docker version: linux-26.1.1:Docker Desktop 4.30.0 (149282)
I1110 09:16:10.040519    7073 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1110 09:16:10.334733    7073 info.go:266] docker info: {ID:8c808b23-7b04-472e-93d2-ad237ae87b54 Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:0 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:68 OomKillDisable:false NGoroutines:107 SystemTime:2024-11-10 06:16:10.313439753 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:27 KernelVersion:6.6.26-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8221949952 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///Users/fkiprop1/Library/Containers/com.docker.docker/Data/docker-cli.sock] ExperimentalBuild:false ServerVersion:26.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:e377cd56a71523140ca6ae87e30244719194a521 Expected:e377cd56a71523140ca6ae87e30244719194a521} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/fkiprop1/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.0-desktop.1] map[Name:compose Path:/Users/fkiprop1/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.0-desktop.2] map[Name:debug Path:/Users/fkiprop1/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.29] map[Name:dev Path:/Users/fkiprop1/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/Users/fkiprop1/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:/Users/fkiprop1/.docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:/Users/fkiprop1/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:/Users/fkiprop1/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/fkiprop1/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.8.0]] Warnings:<nil>}}
I1110 09:16:10.338688    7073 out.go:177] ‚ú®  Using the docker driver based on existing profile
I1110 09:16:10.344911    7073 start.go:297] selected driver: docker
I1110 09:16:10.344916    7073 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1110 09:16:10.345123    7073 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1110 09:16:10.345191    7073 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1110 09:16:10.422282    7073 info.go:266] docker info: {ID:8c808b23-7b04-472e-93d2-ad237ae87b54 Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:0 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:68 OomKillDisable:false NGoroutines:107 SystemTime:2024-11-10 06:16:10.405963087 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:27 KernelVersion:6.6.26-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8221949952 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///Users/fkiprop1/Library/Containers/com.docker.docker/Data/docker-cli.sock] ExperimentalBuild:false ServerVersion:26.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:e377cd56a71523140ca6ae87e30244719194a521 Expected:e377cd56a71523140ca6ae87e30244719194a521} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/fkiprop1/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.0-desktop.1] map[Name:compose Path:/Users/fkiprop1/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.0-desktop.2] map[Name:debug Path:/Users/fkiprop1/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.29] map[Name:dev Path:/Users/fkiprop1/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/Users/fkiprop1/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:/Users/fkiprop1/.docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:/Users/fkiprop1/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:/Users/fkiprop1/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/fkiprop1/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.8.0]] Warnings:<nil>}}
I1110 09:16:10.422914    7073 cni.go:84] Creating CNI manager for ""
I1110 09:16:10.423086    7073 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1110 09:16:10.423305    7073 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1110 09:16:10.427662    7073 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I1110 09:16:10.430910    7073 cache.go:121] Beginning downloading kic base image for docker with docker
I1110 09:16:10.433707    7073 out.go:177] üöú  Pulling base image v0.0.44 ...
I1110 09:16:10.438720    7073 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I1110 09:16:10.438717    7073 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon
I1110 09:16:10.438790    7073 preload.go:147] Found local preload: /Users/fkiprop1/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4
I1110 09:16:10.438976    7073 cache.go:56] Caching tarball of preloaded images
I1110 09:16:10.439533    7073 preload.go:173] Found /Users/fkiprop1/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4 in cache, skipping download
I1110 09:16:10.439558    7073 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I1110 09:16:10.439648    7073 profile.go:143] Saving config to /Users/fkiprop1/.minikube/profiles/minikube/config.json ...
I1110 09:16:10.462622    7073 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e to local cache
I1110 09:16:10.463187    7073 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local cache directory
I1110 09:16:10.463345    7073 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local cache directory, skipping pull
I1110 09:16:10.463499    7073 image.go:105] gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e exists in cache, skipping pull
I1110 09:16:10.463508    7073 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e as a tarball
I1110 09:16:10.463510    7073 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e from local cache
I1110 09:16:29.675054    7073 cache.go:164] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e from cached tarball
I1110 09:16:29.675232    7073 cache.go:194] Successfully downloaded all kic artifacts
I1110 09:16:29.676888    7073 start.go:360] acquireMachinesLock for minikube: {Name:mk88fa91842bef6667e3a4579bb91af8859fbfc8 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1110 09:16:29.677320    7073 start.go:364] duration metric: took 328.375¬µs to acquireMachinesLock for "minikube"
I1110 09:16:29.677392    7073 start.go:96] Skipping create...Using existing machine configuration
I1110 09:16:29.677616    7073 fix.go:54] fixHost starting: 
I1110 09:16:29.678616    7073 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W1110 09:16:29.706605    7073 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I1110 09:16:29.706898    7073 fix.go:112] recreateIfNeeded on minikube: state= err=unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I1110 09:16:29.706925    7073 fix.go:117] machineExists: false. err=machine does not exist
I1110 09:16:29.712203    7073 out.go:177] ü§∑  docker "minikube" container is missing, will recreate.
I1110 09:16:29.719130    7073 delete.go:124] DEMOLISHING minikube ...
I1110 09:16:29.719206    7073 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W1110 09:16:29.747223    7073 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
W1110 09:16:29.747534    7073 stop.go:83] unable to get state: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I1110 09:16:29.747604    7073 delete.go:128] stophost failed (probably ok): ssh power off: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I1110 09:16:29.749826    7073 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W1110 09:16:29.770913    7073 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I1110 09:16:29.770957    7073 delete.go:82] Unable to get host status for minikube, assuming it has already been deleted: state: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I1110 09:16:29.771342    7073 cli_runner.go:164] Run: docker container inspect -f {{.Id}} minikube
W1110 09:16:29.788235    7073 cli_runner.go:211] docker container inspect -f {{.Id}} minikube returned with exit code 1
I1110 09:16:29.788448    7073 kic.go:371] could not find the container minikube to remove it. will try anyways
I1110 09:16:29.788486    7073 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W1110 09:16:29.803424    7073 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
W1110 09:16:29.803457    7073 oci.go:84] error getting container status, will try to delete anyways: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I1110 09:16:29.803498    7073 cli_runner.go:164] Run: docker exec --privileged -t minikube /bin/bash -c "sudo init 0"
W1110 09:16:29.818808    7073 cli_runner.go:211] docker exec --privileged -t minikube /bin/bash -c "sudo init 0" returned with exit code 1
I1110 09:16:29.818825    7073 oci.go:650] error shutdown minikube: docker exec --privileged -t minikube /bin/bash -c "sudo init 0": exit status 1
stdout:

stderr:
Error response from daemon: No such container: minikube
I1110 09:16:30.819121    7073 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W1110 09:16:30.837418    7073 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I1110 09:16:30.837784    7073 oci.go:662] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I1110 09:16:30.837789    7073 oci.go:664] temporary error: container minikube status is  but expect it to be exited
I1110 09:16:30.837821    7073 retry.go:31] will retry after 263.65731ms: couldn't verify container is exited. %!v(MISSING): unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I1110 09:16:31.102502    7073 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W1110 09:16:31.128040    7073 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I1110 09:16:31.128076    7073 oci.go:662] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I1110 09:16:31.128080    7073 oci.go:664] temporary error: container minikube status is  but expect it to be exited
I1110 09:16:31.128101    7073 retry.go:31] will retry after 1.116026868s: couldn't verify container is exited. %!v(MISSING): unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I1110 09:16:32.245489    7073 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W1110 09:16:32.276066    7073 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I1110 09:16:32.276112    7073 oci.go:662] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I1110 09:16:32.276116    7073 oci.go:664] temporary error: container minikube status is  but expect it to be exited
I1110 09:16:32.276134    7073 retry.go:31] will retry after 1.135905926s: couldn't verify container is exited. %!v(MISSING): unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I1110 09:16:33.413610    7073 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W1110 09:16:33.467797    7073 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I1110 09:16:33.467903    7073 oci.go:662] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I1110 09:16:33.467916    7073 oci.go:664] temporary error: container minikube status is  but expect it to be exited
I1110 09:16:33.467975    7073 retry.go:31] will retry after 1.258551165s: couldn't verify container is exited. %!v(MISSING): unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I1110 09:16:34.728289    7073 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W1110 09:16:34.800631    7073 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I1110 09:16:34.800707    7073 oci.go:662] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I1110 09:16:34.800714    7073 oci.go:664] temporary error: container minikube status is  but expect it to be exited
I1110 09:16:34.800748    7073 retry.go:31] will retry after 2.758687449s: couldn't verify container is exited. %!v(MISSING): unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I1110 09:16:37.560370    7073 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W1110 09:16:37.604903    7073 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I1110 09:16:37.604994    7073 oci.go:662] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I1110 09:16:37.605011    7073 oci.go:664] temporary error: container minikube status is  but expect it to be exited
I1110 09:16:37.605071    7073 retry.go:31] will retry after 1.996869559s: couldn't verify container is exited. %!v(MISSING): unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I1110 09:16:39.603423    7073 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W1110 09:16:39.642944    7073 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I1110 09:16:39.643020    7073 oci.go:662] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I1110 09:16:39.643026    7073 oci.go:664] temporary error: container minikube status is  but expect it to be exited
I1110 09:16:39.643069    7073 retry.go:31] will retry after 5.063635317s: couldn't verify container is exited. %!v(MISSING): unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I1110 09:16:44.707922    7073 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W1110 09:16:44.729670    7073 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I1110 09:16:44.729707    7073 oci.go:662] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I1110 09:16:44.729710    7073 oci.go:664] temporary error: container minikube status is  but expect it to be exited
I1110 09:16:44.729727    7073 retry.go:31] will retry after 5.500125793s: couldn't verify container is exited. %!v(MISSING): unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I1110 09:16:50.231407    7073 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W1110 09:16:50.276572    7073 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I1110 09:16:50.276676    7073 oci.go:662] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I1110 09:16:50.276686    7073 oci.go:664] temporary error: container minikube status is  but expect it to be exited
I1110 09:16:50.276732    7073 oci.go:88] couldn't shut down minikube (might be okay): verify shutdown: couldn't verify container is exited. %!v(MISSING): unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
 
I1110 09:16:50.276843    7073 cli_runner.go:164] Run: docker rm -f -v minikube
I1110 09:16:50.313855    7073 cli_runner.go:164] Run: docker container inspect -f {{.Id}} minikube
W1110 09:16:50.340702    7073 cli_runner.go:211] docker container inspect -f {{.Id}} minikube returned with exit code 1
I1110 09:16:50.341017    7073 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1110 09:16:50.367964    7073 cli_runner.go:164] Run: docker network rm minikube
I1110 09:16:50.446257    7073 fix.go:124] Sleeping 1 second for extra luck!
I1110 09:16:51.447434    7073 start.go:125] createHost starting for "" (driver="docker")
I1110 09:16:51.458880    7073 out.go:204] üî•  Creating docker container (CPUs=2, Memory=4000MB) ...
I1110 09:16:51.459789    7073 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I1110 09:16:51.459866    7073 client.go:168] LocalClient.Create starting
I1110 09:16:51.461953    7073 main.go:141] libmachine: Reading certificate data from /Users/fkiprop1/.minikube/certs/ca.pem
I1110 09:16:51.462327    7073 main.go:141] libmachine: Decoding PEM data...
I1110 09:16:51.462402    7073 main.go:141] libmachine: Parsing certificate...
I1110 09:16:51.463589    7073 main.go:141] libmachine: Reading certificate data from /Users/fkiprop1/.minikube/certs/cert.pem
I1110 09:16:51.463914    7073 main.go:141] libmachine: Decoding PEM data...
I1110 09:16:51.463932    7073 main.go:141] libmachine: Parsing certificate...
I1110 09:16:51.465500    7073 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W1110 09:16:51.503006    7073 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I1110 09:16:51.503138    7073 network_create.go:281] running [docker network inspect minikube] to gather additional debugging logs...
I1110 09:16:51.503166    7073 cli_runner.go:164] Run: docker network inspect minikube
W1110 09:16:51.549608    7073 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I1110 09:16:51.549638    7073 network_create.go:284] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I1110 09:16:51.549666    7073 network_create.go:286] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I1110 09:16:51.549841    7073 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1110 09:16:51.583346    7073 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0x14001690410}
I1110 09:16:51.583548    7073 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 65535 ...
I1110 09:16:51.583599    7073 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=65535 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I1110 09:16:51.639746    7073 network_create.go:108] docker network minikube 192.168.49.0/24 created
I1110 09:16:51.639779    7073 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I1110 09:16:51.639864    7073 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I1110 09:16:51.660555    7073 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I1110 09:16:51.682716    7073 oci.go:103] Successfully created a docker volume minikube
I1110 09:16:51.682808    7073 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e -d /var/lib
I1110 09:16:51.987412    7073 oci.go:107] Successfully prepared a docker volume minikube
I1110 09:16:51.987471    7073 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I1110 09:16:51.987509    7073 kic.go:194] Starting extracting preloaded images to volume ...
I1110 09:16:51.987855    7073 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /Users/fkiprop1/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e -I lz4 -xf /preloaded.tar -C /extractDir
I1110 09:16:53.736561    7073 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /Users/fkiprop1/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e -I lz4 -xf /preloaded.tar -C /extractDir: (1.748625625s)
I1110 09:16:53.736609    7073 kic.go:203] duration metric: took 1.749088542s to extract preloaded images to volume ...
I1110 09:16:53.736735    7073 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I1110 09:16:54.024986    7073 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=4000mb --memory-swap=4000mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e
I1110 09:16:54.200743    7073 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I1110 09:16:54.217613    7073 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1110 09:16:54.233840    7073 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I1110 09:16:54.278056    7073 oci.go:144] the created container "minikube" has a running status.
I1110 09:16:54.278090    7073 kic.go:225] Creating ssh key for kic: /Users/fkiprop1/.minikube/machines/minikube/id_rsa...
I1110 09:16:54.361340    7073 kic_runner.go:191] docker (temp): /Users/fkiprop1/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I1110 09:16:54.392910    7073 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1110 09:16:54.417632    7073 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I1110 09:16:54.417651    7073 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I1110 09:16:54.471333    7073 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1110 09:16:54.488960    7073 machine.go:94] provisionDockerMachine start ...
I1110 09:16:54.489053    7073 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 09:16:54.505461    7073 main.go:141] libmachine: Using SSH client type: native
I1110 09:16:54.506067    7073 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1046ff180] 0x1047019e0 <nil>  [] 0s} 127.0.0.1 54864 <nil> <nil>}
I1110 09:16:54.506073    7073 main.go:141] libmachine: About to run SSH command:
hostname
I1110 09:16:54.630452    7073 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1110 09:16:54.631642    7073 ubuntu.go:169] provisioning hostname "minikube"
I1110 09:16:54.631899    7073 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 09:16:54.652595    7073 main.go:141] libmachine: Using SSH client type: native
I1110 09:16:54.652769    7073 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1046ff180] 0x1047019e0 <nil>  [] 0s} 127.0.0.1 54864 <nil> <nil>}
I1110 09:16:54.652773    7073 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1110 09:16:54.788994    7073 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1110 09:16:54.789114    7073 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 09:16:54.823388    7073 main.go:141] libmachine: Using SSH client type: native
I1110 09:16:54.823660    7073 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1046ff180] 0x1047019e0 <nil>  [] 0s} 127.0.0.1 54864 <nil> <nil>}
I1110 09:16:54.823678    7073 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1110 09:16:54.967461    7073 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1110 09:16:54.967493    7073 ubuntu.go:175] set auth options {CertDir:/Users/fkiprop1/.minikube CaCertPath:/Users/fkiprop1/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/fkiprop1/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/fkiprop1/.minikube/machines/server.pem ServerKeyPath:/Users/fkiprop1/.minikube/machines/server-key.pem ClientKeyPath:/Users/fkiprop1/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/fkiprop1/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/fkiprop1/.minikube}
I1110 09:16:54.967539    7073 ubuntu.go:177] setting up certificates
I1110 09:16:54.967563    7073 provision.go:84] configureAuth start
I1110 09:16:54.967737    7073 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1110 09:16:55.004993    7073 provision.go:143] copyHostCerts
I1110 09:16:55.007362    7073 exec_runner.go:144] found /Users/fkiprop1/.minikube/ca.pem, removing ...
I1110 09:16:55.007373    7073 exec_runner.go:203] rm: /Users/fkiprop1/.minikube/ca.pem
I1110 09:16:55.007833    7073 exec_runner.go:151] cp: /Users/fkiprop1/.minikube/certs/ca.pem --> /Users/fkiprop1/.minikube/ca.pem (1082 bytes)
I1110 09:16:55.008409    7073 exec_runner.go:144] found /Users/fkiprop1/.minikube/cert.pem, removing ...
I1110 09:16:55.008412    7073 exec_runner.go:203] rm: /Users/fkiprop1/.minikube/cert.pem
I1110 09:16:55.008794    7073 exec_runner.go:151] cp: /Users/fkiprop1/.minikube/certs/cert.pem --> /Users/fkiprop1/.minikube/cert.pem (1127 bytes)
I1110 09:16:55.009363    7073 exec_runner.go:144] found /Users/fkiprop1/.minikube/key.pem, removing ...
I1110 09:16:55.009368    7073 exec_runner.go:203] rm: /Users/fkiprop1/.minikube/key.pem
I1110 09:16:55.009858    7073 exec_runner.go:151] cp: /Users/fkiprop1/.minikube/certs/key.pem --> /Users/fkiprop1/.minikube/key.pem (1679 bytes)
I1110 09:16:55.010249    7073 provision.go:117] generating server cert: /Users/fkiprop1/.minikube/machines/server.pem ca-key=/Users/fkiprop1/.minikube/certs/ca.pem private-key=/Users/fkiprop1/.minikube/certs/ca-key.pem org=fkiprop1.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I1110 09:16:55.124738    7073 provision.go:177] copyRemoteCerts
I1110 09:16:55.125115    7073 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1110 09:16:55.125140    7073 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 09:16:55.142974    7073 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54864 SSHKeyPath:/Users/fkiprop1/.minikube/machines/minikube/id_rsa Username:docker}
I1110 09:16:55.242831    7073 ssh_runner.go:362] scp /Users/fkiprop1/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1082 bytes)
I1110 09:16:55.265526    7073 ssh_runner.go:362] scp /Users/fkiprop1/.minikube/machines/server.pem --> /etc/docker/server.pem (1184 bytes)
I1110 09:16:55.282284    7073 ssh_runner.go:362] scp /Users/fkiprop1/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1110 09:16:55.294869    7073 provision.go:87] duration metric: took 327.298333ms to configureAuth
I1110 09:16:55.294877    7073 ubuntu.go:193] setting minikube options for container-runtime
I1110 09:16:55.295282    7073 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I1110 09:16:55.295324    7073 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 09:16:55.317665    7073 main.go:141] libmachine: Using SSH client type: native
I1110 09:16:55.317865    7073 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1046ff180] 0x1047019e0 <nil>  [] 0s} 127.0.0.1 54864 <nil> <nil>}
I1110 09:16:55.317869    7073 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1110 09:16:55.450206    7073 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1110 09:16:55.450223    7073 ubuntu.go:71] root file system type: overlay
I1110 09:16:55.450338    7073 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1110 09:16:55.450446    7073 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 09:16:55.481944    7073 main.go:141] libmachine: Using SSH client type: native
I1110 09:16:55.482248    7073 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1046ff180] 0x1047019e0 <nil>  [] 0s} 127.0.0.1 54864 <nil> <nil>}
I1110 09:16:55.482314    7073 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1110 09:16:55.637962    7073 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1110 09:16:55.638241    7073 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 09:16:55.678414    7073 main.go:141] libmachine: Using SSH client type: native
I1110 09:16:55.678755    7073 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1046ff180] 0x1047019e0 <nil>  [] 0s} 127.0.0.1 54864 <nil> <nil>}
I1110 09:16:55.678776    7073 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1110 09:16:56.075344    7073 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-04-30 11:46:21.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2024-11-10 06:16:55.634515011 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I1110 09:16:56.075399    7073 machine.go:97] duration metric: took 1.586411958s to provisionDockerMachine
I1110 09:16:56.075419    7073 client.go:171] duration metric: took 4.615522625s to LocalClient.Create
I1110 09:16:56.075468    7073 start.go:167] duration metric: took 4.615656625s to libmachine.API.Create "minikube"
I1110 09:16:56.075479    7073 start.go:293] postStartSetup for "minikube" (driver="docker")
I1110 09:16:56.075500    7073 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1110 09:16:56.075756    7073 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1110 09:16:56.075857    7073 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 09:16:56.101166    7073 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54864 SSHKeyPath:/Users/fkiprop1/.minikube/machines/minikube/id_rsa Username:docker}
I1110 09:16:56.192230    7073 ssh_runner.go:195] Run: cat /etc/os-release
I1110 09:16:56.194310    7073 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1110 09:16:56.194337    7073 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1110 09:16:56.194355    7073 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1110 09:16:56.194360    7073 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I1110 09:16:56.194368    7073 filesync.go:126] Scanning /Users/fkiprop1/.minikube/addons for local assets ...
I1110 09:16:56.194753    7073 filesync.go:126] Scanning /Users/fkiprop1/.minikube/files for local assets ...
I1110 09:16:56.195000    7073 start.go:296] duration metric: took 119.513416ms for postStartSetup
I1110 09:16:56.197614    7073 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1110 09:16:56.224880    7073 profile.go:143] Saving config to /Users/fkiprop1/.minikube/profiles/minikube/config.json ...
I1110 09:16:56.226109    7073 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1110 09:16:56.226154    7073 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 09:16:56.242856    7073 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54864 SSHKeyPath:/Users/fkiprop1/.minikube/machines/minikube/id_rsa Username:docker}
I1110 09:16:56.330802    7073 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1110 09:16:56.335107    7073 start.go:128] duration metric: took 4.887610541s to createHost
I1110 09:16:56.335185    7073 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W1110 09:16:56.366303    7073 fix.go:138] unexpected machine state, will restart: <nil>
I1110 09:16:56.366338    7073 machine.go:94] provisionDockerMachine start ...
I1110 09:16:56.366470    7073 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 09:16:56.391216    7073 main.go:141] libmachine: Using SSH client type: native
I1110 09:16:56.391438    7073 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1046ff180] 0x1047019e0 <nil>  [] 0s} 127.0.0.1 54864 <nil> <nil>}
I1110 09:16:56.391442    7073 main.go:141] libmachine: About to run SSH command:
hostname
I1110 09:16:56.545116    7073 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1110 09:16:56.545167    7073 ubuntu.go:169] provisioning hostname "minikube"
I1110 09:16:56.545422    7073 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 09:16:56.585404    7073 main.go:141] libmachine: Using SSH client type: native
I1110 09:16:56.585781    7073 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1046ff180] 0x1047019e0 <nil>  [] 0s} 127.0.0.1 54864 <nil> <nil>}
I1110 09:16:56.585790    7073 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1110 09:16:56.761400    7073 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1110 09:16:56.761602    7073 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 09:16:56.801084    7073 main.go:141] libmachine: Using SSH client type: native
I1110 09:16:56.801380    7073 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1046ff180] 0x1047019e0 <nil>  [] 0s} 127.0.0.1 54864 <nil> <nil>}
I1110 09:16:56.801392    7073 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1110 09:16:56.959458    7073 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1110 09:16:56.959502    7073 ubuntu.go:175] set auth options {CertDir:/Users/fkiprop1/.minikube CaCertPath:/Users/fkiprop1/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/fkiprop1/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/fkiprop1/.minikube/machines/server.pem ServerKeyPath:/Users/fkiprop1/.minikube/machines/server-key.pem ClientKeyPath:/Users/fkiprop1/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/fkiprop1/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/fkiprop1/.minikube}
I1110 09:16:56.959527    7073 ubuntu.go:177] setting up certificates
I1110 09:16:56.959543    7073 provision.go:84] configureAuth start
I1110 09:16:56.959726    7073 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1110 09:16:57.010315    7073 provision.go:143] copyHostCerts
I1110 09:16:57.010473    7073 exec_runner.go:144] found /Users/fkiprop1/.minikube/ca.pem, removing ...
I1110 09:16:57.010480    7073 exec_runner.go:203] rm: /Users/fkiprop1/.minikube/ca.pem
I1110 09:16:57.011248    7073 exec_runner.go:151] cp: /Users/fkiprop1/.minikube/certs/ca.pem --> /Users/fkiprop1/.minikube/ca.pem (1082 bytes)
I1110 09:16:57.011739    7073 exec_runner.go:144] found /Users/fkiprop1/.minikube/cert.pem, removing ...
I1110 09:16:57.011743    7073 exec_runner.go:203] rm: /Users/fkiprop1/.minikube/cert.pem
I1110 09:16:57.012562    7073 exec_runner.go:151] cp: /Users/fkiprop1/.minikube/certs/cert.pem --> /Users/fkiprop1/.minikube/cert.pem (1127 bytes)
I1110 09:16:57.012993    7073 exec_runner.go:144] found /Users/fkiprop1/.minikube/key.pem, removing ...
I1110 09:16:57.012998    7073 exec_runner.go:203] rm: /Users/fkiprop1/.minikube/key.pem
I1110 09:16:57.014111    7073 exec_runner.go:151] cp: /Users/fkiprop1/.minikube/certs/key.pem --> /Users/fkiprop1/.minikube/key.pem (1679 bytes)
I1110 09:16:57.014432    7073 provision.go:117] generating server cert: /Users/fkiprop1/.minikube/machines/server.pem ca-key=/Users/fkiprop1/.minikube/certs/ca.pem private-key=/Users/fkiprop1/.minikube/certs/ca-key.pem org=fkiprop1.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I1110 09:16:57.133230    7073 provision.go:177] copyRemoteCerts
I1110 09:16:57.133288    7073 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1110 09:16:57.133315    7073 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 09:16:57.160756    7073 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54864 SSHKeyPath:/Users/fkiprop1/.minikube/machines/minikube/id_rsa Username:docker}
I1110 09:16:57.275695    7073 ssh_runner.go:362] scp /Users/fkiprop1/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1082 bytes)
I1110 09:16:57.298441    7073 ssh_runner.go:362] scp /Users/fkiprop1/.minikube/machines/server.pem --> /etc/docker/server.pem (1184 bytes)
I1110 09:16:57.314503    7073 ssh_runner.go:362] scp /Users/fkiprop1/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1110 09:16:57.327928    7073 provision.go:87] duration metric: took 368.376542ms to configureAuth
I1110 09:16:57.327937    7073 ubuntu.go:193] setting minikube options for container-runtime
I1110 09:16:57.328108    7073 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I1110 09:16:57.328166    7073 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 09:16:57.351929    7073 main.go:141] libmachine: Using SSH client type: native
I1110 09:16:57.352125    7073 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1046ff180] 0x1047019e0 <nil>  [] 0s} 127.0.0.1 54864 <nil> <nil>}
I1110 09:16:57.352140    7073 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1110 09:16:57.485947    7073 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1110 09:16:57.485998    7073 ubuntu.go:71] root file system type: overlay
I1110 09:16:57.486160    7073 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1110 09:16:57.486241    7073 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 09:16:57.513226    7073 main.go:141] libmachine: Using SSH client type: native
I1110 09:16:57.513468    7073 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1046ff180] 0x1047019e0 <nil>  [] 0s} 127.0.0.1 54864 <nil> <nil>}
I1110 09:16:57.513527    7073 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1110 09:16:57.667679    7073 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1110 09:16:57.667764    7073 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 09:16:57.688206    7073 main.go:141] libmachine: Using SSH client type: native
I1110 09:16:57.688378    7073 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1046ff180] 0x1047019e0 <nil>  [] 0s} 127.0.0.1 54864 <nil> <nil>}
I1110 09:16:57.688392    7073 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1110 09:16:57.847899    7073 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1110 09:16:57.847937    7073 machine.go:97] duration metric: took 1.481582208s to provisionDockerMachine
I1110 09:16:57.847954    7073 start.go:293] postStartSetup for "minikube" (driver="docker")
I1110 09:16:57.847972    7073 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1110 09:16:57.848279    7073 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1110 09:16:57.848418    7073 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 09:16:57.886351    7073 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54864 SSHKeyPath:/Users/fkiprop1/.minikube/machines/minikube/id_rsa Username:docker}
I1110 09:16:58.003548    7073 ssh_runner.go:195] Run: cat /etc/os-release
I1110 09:16:58.007574    7073 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1110 09:16:58.007603    7073 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1110 09:16:58.007613    7073 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1110 09:16:58.007621    7073 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I1110 09:16:58.007634    7073 filesync.go:126] Scanning /Users/fkiprop1/.minikube/addons for local assets ...
I1110 09:16:58.007984    7073 filesync.go:126] Scanning /Users/fkiprop1/.minikube/files for local assets ...
I1110 09:16:58.008128    7073 start.go:296] duration metric: took 160.163416ms for postStartSetup
I1110 09:16:58.008317    7073 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1110 09:16:58.008390    7073 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 09:16:58.050881    7073 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54864 SSHKeyPath:/Users/fkiprop1/.minikube/machines/minikube/id_rsa Username:docker}
I1110 09:16:58.162798    7073 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1110 09:16:58.168744    7073 fix.go:56] duration metric: took 28.491194375s for fixHost
I1110 09:16:58.168779    7073 start.go:83] releasing machines lock for "minikube", held for 28.491304125s
I1110 09:16:58.169005    7073 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1110 09:16:58.211247    7073 ssh_runner.go:195] Run: cat /version.json
I1110 09:16:58.211357    7073 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 09:16:58.211842    7073 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1110 09:16:58.212682    7073 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 09:16:58.240940    7073 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54864 SSHKeyPath:/Users/fkiprop1/.minikube/machines/minikube/id_rsa Username:docker}
I1110 09:16:58.244248    7073 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54864 SSHKeyPath:/Users/fkiprop1/.minikube/machines/minikube/id_rsa Username:docker}
W1110 09:16:58.796292    7073 start.go:860] [curl -sS -m 2 https://registry.k8s.io/] failed: curl -sS -m 2 https://registry.k8s.io/: Process exited with status 60
stdout:

stderr:
curl: (60) SSL certificate problem: unable to get local issuer certificate
More details here: https://curl.se/docs/sslcerts.html

curl failed to verify the legitimacy of the server and therefore could not
establish a secure connection to it. To learn more about this situation and
how to fix it, please visit the web page mentioned above.
W1110 09:16:58.796988    7073 out.go:239] ‚ùó  This container is having trouble accessing https://registry.k8s.io
I1110 09:16:58.797031    7073 ssh_runner.go:195] Run: systemctl --version
W1110 09:16:58.797088    7073 out.go:239] üí°  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I1110 09:16:58.801764    7073 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1110 09:16:58.805457    7073 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1110 09:16:58.821619    7073 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1110 09:16:58.821873    7073 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1110 09:16:58.835128    7073 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I1110 09:16:58.835173    7073 start.go:494] detecting cgroup driver to use...
I1110 09:16:58.835196    7073 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1110 09:16:58.835856    7073 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1110 09:16:58.843046    7073 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I1110 09:16:58.846924    7073 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1110 09:16:58.850812    7073 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I1110 09:16:58.850884    7073 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1110 09:16:58.854798    7073 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1110 09:16:58.858359    7073 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1110 09:16:58.861720    7073 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1110 09:16:58.865952    7073 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1110 09:16:58.869313    7073 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1110 09:16:58.873449    7073 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1110 09:16:58.876787    7073 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1110 09:16:58.880715    7073 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1110 09:16:58.883911    7073 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1110 09:16:58.886961    7073 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1110 09:16:58.920999    7073 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1110 09:16:58.965688    7073 start.go:494] detecting cgroup driver to use...
I1110 09:16:58.965705    7073 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1110 09:16:58.966729    7073 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1110 09:16:58.973934    7073 cruntime.go:279] skipping containerd shutdown because we are bound to it
I1110 09:16:58.974049    7073 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1110 09:16:58.979254    7073 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1110 09:16:58.986120    7073 ssh_runner.go:195] Run: which cri-dockerd
I1110 09:16:58.987808    7073 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1110 09:16:58.992229    7073 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I1110 09:16:58.999389    7073 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1110 09:16:59.029632    7073 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1110 09:16:59.070766    7073 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I1110 09:16:59.070968    7073 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1110 09:16:59.077754    7073 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1110 09:16:59.107148    7073 ssh_runner.go:195] Run: sudo systemctl restart docker
I1110 09:16:59.199789    7073 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1110 09:16:59.204605    7073 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1110 09:16:59.209345    7073 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1110 09:16:59.240537    7073 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1110 09:16:59.270541    7073 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1110 09:16:59.305555    7073 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1110 09:16:59.325151    7073 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1110 09:16:59.330636    7073 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1110 09:16:59.362152    7073 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1110 09:16:59.409610    7073 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1110 09:16:59.410821    7073 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1110 09:16:59.412767    7073 start.go:562] Will wait 60s for crictl version
I1110 09:16:59.412891    7073 ssh_runner.go:195] Run: which crictl
I1110 09:16:59.414746    7073 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1110 09:16:59.436705    7073 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.1.1
RuntimeApiVersion:  v1
I1110 09:16:59.436772    7073 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1110 09:16:59.451995    7073 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1110 09:16:59.468958    7073 out.go:204] üê≥  Preparing Kubernetes v1.30.0 on Docker 26.1.1 ...
I1110 09:16:59.469180    7073 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I1110 09:16:59.549645    7073 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I1110 09:16:59.551271    7073 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I1110 09:16:59.553578    7073 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1110 09:16:59.558154    7073 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1110 09:16:59.583177    7073 kubeadm.go:877] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1110 09:16:59.583348    7073 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I1110 09:16:59.583399    7073 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1110 09:16:59.592059    7073 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1110 09:16:59.592065    7073 docker.go:615] Images already preloaded, skipping extraction
I1110 09:16:59.592322    7073 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1110 09:16:59.598717    7073 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1110 09:16:59.598725    7073 cache_images.go:84] Images are preloaded, skipping loading
I1110 09:16:59.598730    7073 kubeadm.go:928] updating node { 192.168.49.2 8443 v1.30.0 docker true true} ...
I1110 09:16:59.598790    7073 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1110 09:16:59.598813    7073 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1110 09:16:59.627308    7073 cni.go:84] Creating CNI manager for ""
I1110 09:16:59.627322    7073 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1110 09:16:59.627334    7073 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1110 09:16:59.627354    7073 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.30.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1110 09:16:59.627500    7073 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.30.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1110 09:16:59.627619    7073 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I1110 09:16:59.631191    7073 binaries.go:44] Found k8s binaries, skipping transfer
I1110 09:16:59.631276    7073 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1110 09:16:59.634569    7073 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I1110 09:16:59.641100    7073 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1110 09:16:59.648188    7073 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I1110 09:16:59.654194    7073 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1110 09:16:59.655718    7073 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1110 09:16:59.659570    7073 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1110 09:16:59.689294    7073 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1110 09:16:59.705874    7073 certs.go:68] Setting up /Users/fkiprop1/.minikube/profiles/minikube for IP: 192.168.49.2
I1110 09:16:59.706087    7073 certs.go:194] generating shared ca certs ...
I1110 09:16:59.706105    7073 certs.go:226] acquiring lock for ca certs: {Name:mk5f72eac4a7b822301dbdbe15bd6be26ad5ceee Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1110 09:16:59.707829    7073 certs.go:235] skipping valid "minikubeCA" ca cert: /Users/fkiprop1/.minikube/ca.key
I1110 09:16:59.708264    7073 certs.go:235] skipping valid "proxyClientCA" ca cert: /Users/fkiprop1/.minikube/proxy-client-ca.key
I1110 09:16:59.708284    7073 certs.go:256] generating profile certs ...
I1110 09:16:59.708841    7073 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /Users/fkiprop1/.minikube/profiles/minikube/client.key
I1110 09:16:59.709243    7073 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /Users/fkiprop1/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I1110 09:16:59.709631    7073 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /Users/fkiprop1/.minikube/profiles/minikube/proxy-client.key
I1110 09:16:59.710250    7073 certs.go:484] found cert: /Users/fkiprop1/.minikube/certs/ca-key.pem (1675 bytes)
I1110 09:16:59.710345    7073 certs.go:484] found cert: /Users/fkiprop1/.minikube/certs/ca.pem (1082 bytes)
I1110 09:16:59.710421    7073 certs.go:484] found cert: /Users/fkiprop1/.minikube/certs/cert.pem (1127 bytes)
I1110 09:16:59.710489    7073 certs.go:484] found cert: /Users/fkiprop1/.minikube/certs/key.pem (1679 bytes)
I1110 09:16:59.712095    7073 ssh_runner.go:362] scp /Users/fkiprop1/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1110 09:16:59.721948    7073 ssh_runner.go:362] scp /Users/fkiprop1/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1110 09:16:59.731508    7073 ssh_runner.go:362] scp /Users/fkiprop1/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1110 09:16:59.741329    7073 ssh_runner.go:362] scp /Users/fkiprop1/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1110 09:16:59.750334    7073 ssh_runner.go:362] scp /Users/fkiprop1/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1110 09:16:59.759015    7073 ssh_runner.go:362] scp /Users/fkiprop1/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1110 09:16:59.766921    7073 ssh_runner.go:362] scp /Users/fkiprop1/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1110 09:16:59.774709    7073 ssh_runner.go:362] scp /Users/fkiprop1/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I1110 09:16:59.782687    7073 ssh_runner.go:362] scp /Users/fkiprop1/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1110 09:16:59.790603    7073 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1110 09:16:59.797498    7073 ssh_runner.go:195] Run: openssl version
I1110 09:16:59.800143    7073 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1110 09:16:59.805407    7073 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1110 09:16:59.807226    7073 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Sep 18 09:38 /usr/share/ca-certificates/minikubeCA.pem
I1110 09:16:59.807303    7073 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1110 09:16:59.810230    7073 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1110 09:16:59.813516    7073 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1110 09:16:59.814963    7073 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I1110 09:16:59.815264    7073 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1110 09:16:59.815424    7073 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1110 09:16:59.821632    7073 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1110 09:16:59.824810    7073 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1110 09:16:59.828201    7073 kubeadm.go:213] ignoring SystemVerification for kubeadm because of docker driver
I1110 09:16:59.828316    7073 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1110 09:16:59.831575    7073 kubeadm.go:154] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I1110 09:16:59.831589    7073 kubeadm.go:156] found existing configuration files:

I1110 09:16:59.831692    7073 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1110 09:16:59.834960    7073 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I1110 09:16:59.835072    7073 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I1110 09:16:59.839475    7073 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1110 09:16:59.842633    7073 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I1110 09:16:59.842675    7073 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I1110 09:16:59.845693    7073 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1110 09:16:59.848808    7073 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I1110 09:16:59.848852    7073 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1110 09:16:59.851955    7073 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1110 09:16:59.855296    7073 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I1110 09:16:59.855387    7073 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1110 09:16:59.858603    7073 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I1110 09:16:59.891841    7073 kubeadm.go:309] 	[WARNING Swap]: swap is supported for cgroup v2 only; the NodeSwap feature gate of the kubelet is beta but disabled by default
I1110 09:16:59.919712    7073 kubeadm.go:309] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I1110 09:17:06.225108    7073 kubeadm.go:309] [init] Using Kubernetes version: v1.30.0
I1110 09:17:06.225205    7073 kubeadm.go:309] [preflight] Running pre-flight checks
I1110 09:17:06.225334    7073 kubeadm.go:309] [preflight] Pulling images required for setting up a Kubernetes cluster
I1110 09:17:06.225468    7073 kubeadm.go:309] [preflight] This might take a minute or two, depending on the speed of your internet connection
I1110 09:17:06.225591    7073 kubeadm.go:309] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I1110 09:17:06.225669    7073 kubeadm.go:309] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I1110 09:17:06.235904    7073 out.go:204]     ‚ñ™ Generating certificates and keys ...
I1110 09:17:06.236082    7073 kubeadm.go:309] [certs] Using existing ca certificate authority
I1110 09:17:06.236184    7073 kubeadm.go:309] [certs] Using existing apiserver certificate and key on disk
I1110 09:17:06.236308    7073 kubeadm.go:309] [certs] Generating "apiserver-kubelet-client" certificate and key
I1110 09:17:06.236427    7073 kubeadm.go:309] [certs] Generating "front-proxy-ca" certificate and key
I1110 09:17:06.236524    7073 kubeadm.go:309] [certs] Generating "front-proxy-client" certificate and key
I1110 09:17:06.236592    7073 kubeadm.go:309] [certs] Generating "etcd/ca" certificate and key
I1110 09:17:06.236699    7073 kubeadm.go:309] [certs] Generating "etcd/server" certificate and key
I1110 09:17:06.236895    7073 kubeadm.go:309] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1110 09:17:06.236977    7073 kubeadm.go:309] [certs] Generating "etcd/peer" certificate and key
I1110 09:17:06.237245    7073 kubeadm.go:309] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1110 09:17:06.237435    7073 kubeadm.go:309] [certs] Generating "etcd/healthcheck-client" certificate and key
I1110 09:17:06.237537    7073 kubeadm.go:309] [certs] Generating "apiserver-etcd-client" certificate and key
I1110 09:17:06.237608    7073 kubeadm.go:309] [certs] Generating "sa" key and public key
I1110 09:17:06.237711    7073 kubeadm.go:309] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I1110 09:17:06.237784    7073 kubeadm.go:309] [kubeconfig] Writing "admin.conf" kubeconfig file
I1110 09:17:06.237880    7073 kubeadm.go:309] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I1110 09:17:06.237980    7073 kubeadm.go:309] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I1110 09:17:06.238080    7073 kubeadm.go:309] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I1110 09:17:06.238180    7073 kubeadm.go:309] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I1110 09:17:06.238400    7073 kubeadm.go:309] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I1110 09:17:06.238491    7073 kubeadm.go:309] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I1110 09:17:06.244886    7073 out.go:204]     ‚ñ™ Booting up control plane ...
I1110 09:17:06.245061    7073 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-apiserver"
I1110 09:17:06.245187    7073 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I1110 09:17:06.245324    7073 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-scheduler"
I1110 09:17:06.245475    7073 kubeadm.go:309] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I1110 09:17:06.245688    7073 kubeadm.go:309] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I1110 09:17:06.245749    7073 kubeadm.go:309] [kubelet-start] Starting the kubelet
I1110 09:17:06.245965    7073 kubeadm.go:309] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I1110 09:17:06.246076    7073 kubeadm.go:309] [kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s
I1110 09:17:06.246174    7073 kubeadm.go:309] [kubelet-check] The kubelet is healthy after 504.453167ms
I1110 09:17:06.246313    7073 kubeadm.go:309] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I1110 09:17:06.246402    7073 kubeadm.go:309] [api-check] The API server is healthy after 3.001518043s
I1110 09:17:06.246560    7073 kubeadm.go:309] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I1110 09:17:06.246743    7073 kubeadm.go:309] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I1110 09:17:06.246835    7073 kubeadm.go:309] [upload-certs] Skipping phase. Please see --upload-certs
I1110 09:17:06.247126    7073 kubeadm.go:309] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I1110 09:17:06.247221    7073 kubeadm.go:309] [bootstrap-token] Using token: 74022r.bdynrmc72pe01qkt
I1110 09:17:06.252235    7073 out.go:204]     ‚ñ™ Configuring RBAC rules ...
I1110 09:17:06.253851    7073 kubeadm.go:309] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I1110 09:17:06.254018    7073 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I1110 09:17:06.254465    7073 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I1110 09:17:06.254729    7073 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I1110 09:17:06.254967    7073 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I1110 09:17:06.255159    7073 kubeadm.go:309] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I1110 09:17:06.255378    7073 kubeadm.go:309] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I1110 09:17:06.255454    7073 kubeadm.go:309] [addons] Applied essential addon: CoreDNS
I1110 09:17:06.255540    7073 kubeadm.go:309] [addons] Applied essential addon: kube-proxy
I1110 09:17:06.255546    7073 kubeadm.go:309] 
I1110 09:17:06.255656    7073 kubeadm.go:309] Your Kubernetes control-plane has initialized successfully!
I1110 09:17:06.255689    7073 kubeadm.go:309] 
I1110 09:17:06.255832    7073 kubeadm.go:309] To start using your cluster, you need to run the following as a regular user:
I1110 09:17:06.255840    7073 kubeadm.go:309] 
I1110 09:17:06.255878    7073 kubeadm.go:309]   mkdir -p $HOME/.kube
I1110 09:17:06.256009    7073 kubeadm.go:309]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I1110 09:17:06.256097    7073 kubeadm.go:309]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I1110 09:17:06.256121    7073 kubeadm.go:309] 
I1110 09:17:06.256214    7073 kubeadm.go:309] Alternatively, if you are the root user, you can run:
I1110 09:17:06.256222    7073 kubeadm.go:309] 
I1110 09:17:06.256301    7073 kubeadm.go:309]   export KUBECONFIG=/etc/kubernetes/admin.conf
I1110 09:17:06.256310    7073 kubeadm.go:309] 
I1110 09:17:06.256404    7073 kubeadm.go:309] You should now deploy a pod network to the cluster.
I1110 09:17:06.256501    7073 kubeadm.go:309] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I1110 09:17:06.256606    7073 kubeadm.go:309]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I1110 09:17:06.256609    7073 kubeadm.go:309] 
I1110 09:17:06.256714    7073 kubeadm.go:309] You can now join any number of control-plane nodes by copying certificate authorities
I1110 09:17:06.256828    7073 kubeadm.go:309] and service account keys on each node and then running the following as root:
I1110 09:17:06.256834    7073 kubeadm.go:309] 
I1110 09:17:06.256964    7073 kubeadm.go:309]   kubeadm join control-plane.minikube.internal:8443 --token 74022r.bdynrmc72pe01qkt \
I1110 09:17:06.257120    7073 kubeadm.go:309] 	--discovery-token-ca-cert-hash sha256:569dad87c4d75b7ddb309ba3e5bfdb9748f49797f06522761513475188ca9d85 \
I1110 09:17:06.257154    7073 kubeadm.go:309] 	--control-plane 
I1110 09:17:06.257159    7073 kubeadm.go:309] 
I1110 09:17:06.257286    7073 kubeadm.go:309] Then you can join any number of worker nodes by running the following on each as root:
I1110 09:17:06.257290    7073 kubeadm.go:309] 
I1110 09:17:06.257405    7073 kubeadm.go:309] kubeadm join control-plane.minikube.internal:8443 --token 74022r.bdynrmc72pe01qkt \
I1110 09:17:06.257552    7073 kubeadm.go:309] 	--discovery-token-ca-cert-hash sha256:569dad87c4d75b7ddb309ba3e5bfdb9748f49797f06522761513475188ca9d85 
I1110 09:17:06.257625    7073 cni.go:84] Creating CNI manager for ""
I1110 09:17:06.257642    7073 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1110 09:17:06.261912    7073 out.go:177] üîó  Configuring bridge CNI (Container Networking Interface) ...
I1110 09:17:06.268160    7073 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I1110 09:17:06.288766    7073 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I1110 09:17:06.308809    7073 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1110 09:17:06.308935    7073 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.30.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I1110 09:17:06.309251    7073 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2024_11_10T09_17_06_0700 minikube.k8s.io/version=v1.33.1 minikube.k8s.io/commit=5883c09216182566a63dff4c326a6fc9ed2982ff minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I1110 09:17:06.317738    7073 ops.go:34] apiserver oom_adj: -16
I1110 09:17:06.380916    7073 kubeadm.go:1107] duration metric: took 72.105583ms to wait for elevateKubeSystemPrivileges
W1110 09:17:06.380943    7073 kubeadm.go:286] apiserver tunnel failed: apiserver port not set
I1110 09:17:06.380966    7073 kubeadm.go:393] duration metric: took 6.565678417s to StartCluster
I1110 09:17:06.380998    7073 settings.go:142] acquiring lock: {Name:mk65754c40c1e487d066e4712f1797f08e60a135 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1110 09:17:06.381206    7073 settings.go:150] Updating kubeconfig:  /Users/fkiprop1/.kube/config
I1110 09:17:06.383837    7073 lock.go:35] WriteFile acquiring /Users/fkiprop1/.kube/config: {Name:mk4969362e0e76ea2ba295d614d7a20efad8e6af Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1110 09:17:06.384889    7073 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I1110 09:17:06.385065    7073 start.go:234] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1110 09:17:06.388895    7073 out.go:177] üîé  Verifying Kubernetes components...
I1110 09:17:06.385028    7073 addons.go:502] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:true nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I1110 09:17:06.388960    7073 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1110 09:17:06.388983    7073 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1110 09:17:06.388991    7073 addons.go:69] Setting dashboard=true in profile "minikube"
I1110 09:17:06.389002    7073 addons.go:69] Setting metrics-server=true in profile "minikube"
I1110 09:17:06.395089    7073 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1110 09:17:06.395195    7073 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1110 09:17:06.395280    7073 addons.go:234] Setting addon metrics-server=true in "minikube"
I1110 09:17:06.395275    7073 addons.go:234] Setting addon storage-provisioner=true in "minikube"
W1110 09:17:06.395292    7073 addons.go:243] addon metrics-server should already be in state true
W1110 09:17:06.395295    7073 addons.go:243] addon storage-provisioner should already be in state true
I1110 09:17:06.395357    7073 host.go:66] Checking if "minikube" exists ...
I1110 09:17:06.395360    7073 host.go:66] Checking if "minikube" exists ...
I1110 09:17:06.395584    7073 addons.go:234] Setting addon dashboard=true in "minikube"
W1110 09:17:06.395597    7073 addons.go:243] addon dashboard should already be in state true
I1110 09:17:06.395669    7073 host.go:66] Checking if "minikube" exists ...
I1110 09:17:06.397063    7073 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1110 09:17:06.398940    7073 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1110 09:17:06.399883    7073 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1110 09:17:06.399886    7073 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1110 09:17:06.427013    7073 out.go:177]     ‚ñ™ Using image docker.io/kubernetesui/dashboard:v2.7.0
I1110 09:17:06.431891    7073 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1110 09:17:06.435411    7073 out.go:177]     ‚ñ™ Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I1110 09:17:06.434259    7073 addons.go:426] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1110 09:17:06.439167    7073 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1110 09:17:06.439185    7073 addons.go:426] installing /etc/kubernetes/addons/dashboard-ns.yaml
I1110 09:17:06.439192    7073 ssh_runner.go:362] scp dashboard/dashboard-ns.yaml --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I1110 09:17:06.439225    7073 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 09:17:06.439233    7073 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 09:17:06.444062    7073 out.go:177]     ‚ñ™ Using image registry.k8s.io/metrics-server/metrics-server:v0.7.1
I1110 09:17:06.448063    7073 addons.go:426] installing /etc/kubernetes/addons/metrics-apiservice.yaml
I1110 09:17:06.448068    7073 ssh_runner.go:362] scp metrics-server/metrics-apiservice.yaml --> /etc/kubernetes/addons/metrics-apiservice.yaml (424 bytes)
I1110 09:17:06.445005    7073 addons.go:234] Setting addon default-storageclass=true in "minikube"
I1110 09:17:06.448133    7073 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
W1110 09:17:06.448133    7073 addons.go:243] addon default-storageclass should already be in state true
I1110 09:17:06.448158    7073 host.go:66] Checking if "minikube" exists ...
I1110 09:17:06.448525    7073 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1110 09:17:06.449279    7073 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1110 09:17:06.461317    7073 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1110 09:17:06.462676    7073 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54864 SSHKeyPath:/Users/fkiprop1/.minikube/machines/minikube/id_rsa Username:docker}
I1110 09:17:06.463136    7073 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54864 SSHKeyPath:/Users/fkiprop1/.minikube/machines/minikube/id_rsa Username:docker}
I1110 09:17:06.466124    7073 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54864 SSHKeyPath:/Users/fkiprop1/.minikube/machines/minikube/id_rsa Username:docker}
I1110 09:17:06.467339    7073 addons.go:426] installing /etc/kubernetes/addons/storageclass.yaml
I1110 09:17:06.467349    7073 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1110 09:17:06.467430    7073 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 09:17:06.481255    7073 api_server.go:52] waiting for apiserver process to appear ...
I1110 09:17:06.481311    7073 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1110 09:17:06.483118    7073 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54864 SSHKeyPath:/Users/fkiprop1/.minikube/machines/minikube/id_rsa Username:docker}
I1110 09:17:06.487001    7073 api_server.go:72] duration metric: took 101.918041ms to wait for apiserver process to appear ...
I1110 09:17:06.487006    7073 api_server.go:88] waiting for apiserver healthz status ...
I1110 09:17:06.487224    7073 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:54868/healthz ...
I1110 09:17:06.490922    7073 api_server.go:279] https://127.0.0.1:54868/healthz returned 200:
ok
I1110 09:17:06.495370    7073 api_server.go:141] control plane version: v1.30.0
I1110 09:17:06.495375    7073 api_server.go:131] duration metric: took 8.367417ms to wait for apiserver health ...
I1110 09:17:06.495378    7073 system_pods.go:43] waiting for kube-system pods to appear ...
I1110 09:17:06.501356    7073 system_pods.go:59] 4 kube-system pods found
I1110 09:17:06.501367    7073 system_pods.go:61] "etcd-minikube" [63247b01-391b-4588-acb0-16ba8cabce75] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1110 09:17:06.501371    7073 system_pods.go:61] "kube-apiserver-minikube" [1fe78398-5d6c-48b0-9a05-4cfaffabec4f] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1110 09:17:06.501375    7073 system_pods.go:61] "kube-controller-manager-minikube" [f2a0ebe3-801f-41a1-a2e4-1ecd49d6ff9c] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1110 09:17:06.501377    7073 system_pods.go:61] "kube-scheduler-minikube" [d38fa487-452b-4e84-a2d8-5cb11a6e892b] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1110 09:17:06.501379    7073 system_pods.go:74] duration metric: took 5.999209ms to wait for pod list to return data ...
I1110 09:17:06.501383    7073 kubeadm.go:576] duration metric: took 116.301708ms to wait for: map[apiserver:true system_pods:true]
I1110 09:17:06.501388    7073 node_conditions.go:102] verifying NodePressure condition ...
I1110 09:17:06.502930    7073 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki
I1110 09:17:06.502938    7073 node_conditions.go:123] node cpu capacity is 8
I1110 09:17:06.502943    7073 node_conditions.go:105] duration metric: took 1.553625ms to run NodePressure ...
I1110 09:17:06.502948    7073 start.go:240] waiting for startup goroutines ...
I1110 09:17:06.557016    7073 addons.go:426] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I1110 09:17:06.557022    7073 ssh_runner.go:362] scp dashboard/dashboard-clusterrole.yaml --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I1110 09:17:06.557174    7073 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1110 09:17:06.559764    7073 addons.go:426] installing /etc/kubernetes/addons/metrics-server-deployment.yaml
I1110 09:17:06.559767    7073 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-deployment.yaml (1907 bytes)
I1110 09:17:06.566653    7073 addons.go:426] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I1110 09:17:06.566657    7073 ssh_runner.go:362] scp dashboard/dashboard-clusterrolebinding.yaml --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I1110 09:17:06.569402    7073 addons.go:426] installing /etc/kubernetes/addons/metrics-server-rbac.yaml
I1110 09:17:06.569406    7073 ssh_runner.go:362] scp metrics-server/metrics-server-rbac.yaml --> /etc/kubernetes/addons/metrics-server-rbac.yaml (2175 bytes)
I1110 09:17:06.571730    7073 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1110 09:17:06.575725    7073 addons.go:426] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I1110 09:17:06.575729    7073 ssh_runner.go:362] scp dashboard/dashboard-configmap.yaml --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I1110 09:17:06.581826    7073 addons.go:426] installing /etc/kubernetes/addons/metrics-server-service.yaml
I1110 09:17:06.581831    7073 ssh_runner.go:362] scp metrics-server/metrics-server-service.yaml --> /etc/kubernetes/addons/metrics-server-service.yaml (446 bytes)
I1110 09:17:06.585844    7073 addons.go:426] installing /etc/kubernetes/addons/dashboard-dp.yaml
I1110 09:17:06.585848    7073 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I1110 09:17:06.591949    7073 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
I1110 09:17:06.597900    7073 addons.go:426] installing /etc/kubernetes/addons/dashboard-role.yaml
I1110 09:17:06.597904    7073 ssh_runner.go:362] scp dashboard/dashboard-role.yaml --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I1110 09:17:06.612786    7073 addons.go:426] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I1110 09:17:06.612794    7073 ssh_runner.go:362] scp dashboard/dashboard-rolebinding.yaml --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I1110 09:17:06.674528    7073 addons.go:426] installing /etc/kubernetes/addons/dashboard-sa.yaml
I1110 09:17:06.674541    7073 ssh_runner.go:362] scp dashboard/dashboard-sa.yaml --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I1110 09:17:06.684530    7073 addons.go:426] installing /etc/kubernetes/addons/dashboard-secret.yaml
I1110 09:17:06.684543    7073 ssh_runner.go:362] scp dashboard/dashboard-secret.yaml --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I1110 09:17:06.692305    7073 addons.go:426] installing /etc/kubernetes/addons/dashboard-svc.yaml
I1110 09:17:06.692324    7073 ssh_runner.go:362] scp dashboard/dashboard-svc.yaml --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I1110 09:17:06.701649    7073 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I1110 09:17:06.891469    7073 addons.go:470] Verifying addon metrics-server=true in "minikube"
I1110 09:17:07.068571    7073 out.go:177] üí°  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server

I1110 09:17:07.072630    7073 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass, metrics-server, dashboard
I1110 09:17:07.078581    7073 addons.go:505] duration metric: took 693.678792ms for enable addons: enabled=[storage-provisioner default-storageclass metrics-server dashboard]
I1110 09:17:07.078610    7073 start.go:245] waiting for cluster config update ...
I1110 09:17:07.078622    7073 start.go:254] writing updated cluster config ...
I1110 09:17:07.079414    7073 ssh_runner.go:195] Run: rm -f paused
I1110 09:17:07.272488    7073 start.go:600] kubectl: 1.31.0, cluster: 1.30.0 (minor skew: 1)
I1110 09:17:07.274504    7073 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Nov 10 06:25:32 minikube dockerd[1181]: time="2024-11-10T06:25:32.658043791Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=7f44118833f1a8bc traceID=e7ae18515100455e0f25b82c5c7532cc
Nov 10 06:25:32 minikube dockerd[1181]: time="2024-11-10T06:25:32.658503666Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=7f44118833f1a8bc traceID=e7ae18515100455e0f25b82c5c7532cc
Nov 10 06:25:32 minikube dockerd[1181]: time="2024-11-10T06:25:32.663639125Z" level=error msg="Handler for POST /v1.44/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=7f44118833f1a8bc traceID=e7ae18515100455e0f25b82c5c7532cc
Nov 10 06:26:26 minikube dockerd[1181]: time="2024-11-10T06:26:26.669097553Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=65846882528c8ab1 traceID=d02c4a92d546bfcc912488e896c54874
Nov 10 06:26:26 minikube dockerd[1181]: time="2024-11-10T06:26:26.669492553Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=65846882528c8ab1 traceID=d02c4a92d546bfcc912488e896c54874
Nov 10 06:26:26 minikube dockerd[1181]: time="2024-11-10T06:26:26.675159219Z" level=error msg="Handler for POST /v1.44/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=65846882528c8ab1 traceID=d02c4a92d546bfcc912488e896c54874
Nov 10 06:27:53 minikube dockerd[1181]: time="2024-11-10T06:27:53.664514426Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=f0be33dfe56c63a3 traceID=ca7a0d97855b3d04b005b1dd48cb3036
Nov 10 06:27:53 minikube dockerd[1181]: time="2024-11-10T06:27:53.665115010Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=f0be33dfe56c63a3 traceID=ca7a0d97855b3d04b005b1dd48cb3036
Nov 10 06:27:53 minikube dockerd[1181]: time="2024-11-10T06:27:53.672508760Z" level=error msg="Handler for POST /v1.44/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=f0be33dfe56c63a3 traceID=ca7a0d97855b3d04b005b1dd48cb3036
Nov 10 06:28:15 minikube dockerd[1181]: time="2024-11-10T06:28:15.019570256Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=fbaf1aae060632d1 traceID=3e0a23dec35ec578e109bcdb5531b252
Nov 10 06:28:15 minikube dockerd[1181]: time="2024-11-10T06:28:15.019840089Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=fbaf1aae060632d1 traceID=3e0a23dec35ec578e109bcdb5531b252
Nov 10 06:28:15 minikube dockerd[1181]: time="2024-11-10T06:28:15.023710172Z" level=error msg="Handler for POST /v1.44/images/create returned error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=fbaf1aae060632d1 traceID=3e0a23dec35ec578e109bcdb5531b252
Nov 10 06:28:15 minikube dockerd[1181]: time="2024-11-10T06:28:15.656398547Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=35fc671cfc4fee6a traceID=f11f56129f8e207de3449aebf5f91110
Nov 10 06:28:15 minikube dockerd[1181]: time="2024-11-10T06:28:15.657084839Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=35fc671cfc4fee6a traceID=f11f56129f8e207de3449aebf5f91110
Nov 10 06:28:15 minikube dockerd[1181]: time="2024-11-10T06:28:15.661205506Z" level=error msg="Handler for POST /v1.44/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=35fc671cfc4fee6a traceID=f11f56129f8e207de3449aebf5f91110
Nov 10 06:28:19 minikube dockerd[1181]: time="2024-11-10T06:28:19.647882549Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=b9442fdc8fbcd8e9 traceID=aa89bb92a2bfdff9dd9d4ec5c6637209
Nov 10 06:28:19 minikube dockerd[1181]: time="2024-11-10T06:28:19.648992466Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=b9442fdc8fbcd8e9 traceID=aa89bb92a2bfdff9dd9d4ec5c6637209
Nov 10 06:28:19 minikube dockerd[1181]: time="2024-11-10T06:28:19.654768091Z" level=error msg="Handler for POST /v1.44/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=b9442fdc8fbcd8e9 traceID=aa89bb92a2bfdff9dd9d4ec5c6637209
Nov 10 06:28:23 minikube dockerd[1181]: time="2024-11-10T06:28:23.664720468Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=1a5a44a1ea18a6ac traceID=5335ce62577cab5d5115d99173b380fd
Nov 10 06:28:23 minikube dockerd[1181]: time="2024-11-10T06:28:23.665176385Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=1a5a44a1ea18a6ac traceID=5335ce62577cab5d5115d99173b380fd
Nov 10 06:28:23 minikube dockerd[1181]: time="2024-11-10T06:28:23.670341135Z" level=error msg="Handler for POST /v1.44/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=1a5a44a1ea18a6ac traceID=5335ce62577cab5d5115d99173b380fd
Nov 10 06:28:23 minikube dockerd[1181]: time="2024-11-10T06:28:23.961413718Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=4bd0afcf35da6ebf traceID=ac94c82ab338212b5d529ebcb5acddf5
Nov 10 06:28:23 minikube dockerd[1181]: time="2024-11-10T06:28:23.961594593Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=4bd0afcf35da6ebf traceID=ac94c82ab338212b5d529ebcb5acddf5
Nov 10 06:28:23 minikube dockerd[1181]: time="2024-11-10T06:28:23.964044051Z" level=error msg="Handler for POST /v1.44/images/create returned error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=4bd0afcf35da6ebf traceID=ac94c82ab338212b5d529ebcb5acddf5
Nov 10 06:30:40 minikube dockerd[1181]: time="2024-11-10T06:30:40.666257420Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=9caac81391fde212 traceID=4dd06ce5a6a63b3f59edcf7a6cff383e
Nov 10 06:30:40 minikube dockerd[1181]: time="2024-11-10T06:30:40.666592295Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=9caac81391fde212 traceID=4dd06ce5a6a63b3f59edcf7a6cff383e
Nov 10 06:30:40 minikube dockerd[1181]: time="2024-11-10T06:30:40.671623878Z" level=error msg="Handler for POST /v1.44/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=9caac81391fde212 traceID=4dd06ce5a6a63b3f59edcf7a6cff383e
Nov 10 06:33:18 minikube dockerd[1181]: time="2024-11-10T06:33:18.091403590Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=9b32ad27d5b21cb6 traceID=f0cec5f4eabff13d2207a296b749c8e2
Nov 10 06:33:18 minikube dockerd[1181]: time="2024-11-10T06:33:18.091523174Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=9b32ad27d5b21cb6 traceID=f0cec5f4eabff13d2207a296b749c8e2
Nov 10 06:33:18 minikube dockerd[1181]: time="2024-11-10T06:33:18.096364257Z" level=error msg="Handler for POST /v1.44/images/create returned error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=9b32ad27d5b21cb6 traceID=f0cec5f4eabff13d2207a296b749c8e2
Nov 10 06:33:24 minikube dockerd[1181]: time="2024-11-10T06:33:24.655292302Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=c00f4e6a95af4596 traceID=a2b22c248e31978ced73338ff876f2ff
Nov 10 06:33:24 minikube dockerd[1181]: time="2024-11-10T06:33:24.655491385Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=c00f4e6a95af4596 traceID=a2b22c248e31978ced73338ff876f2ff
Nov 10 06:33:24 minikube dockerd[1181]: time="2024-11-10T06:33:24.658317468Z" level=error msg="Handler for POST /v1.44/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=c00f4e6a95af4596 traceID=a2b22c248e31978ced73338ff876f2ff
Nov 10 06:33:24 minikube dockerd[1181]: time="2024-11-10T06:33:24.940770010Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=6b951edddd7b5e2e traceID=11d6458d6c61939875663d0127541df4
Nov 10 06:33:24 minikube dockerd[1181]: time="2024-11-10T06:33:24.941796010Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=6b951edddd7b5e2e traceID=11d6458d6c61939875663d0127541df4
Nov 10 06:33:24 minikube dockerd[1181]: time="2024-11-10T06:33:24.945719927Z" level=error msg="Handler for POST /v1.44/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=6b951edddd7b5e2e traceID=11d6458d6c61939875663d0127541df4
Nov 10 06:33:29 minikube dockerd[1181]: time="2024-11-10T06:33:29.644912096Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=3f759b4546191a28 traceID=b8d6e190ac217722f18f7da2ce238663
Nov 10 06:33:29 minikube dockerd[1181]: time="2024-11-10T06:33:29.645274262Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=3f759b4546191a28 traceID=b8d6e190ac217722f18f7da2ce238663
Nov 10 06:33:29 minikube dockerd[1181]: time="2024-11-10T06:33:29.648856137Z" level=error msg="Handler for POST /v1.44/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=3f759b4546191a28 traceID=b8d6e190ac217722f18f7da2ce238663
Nov 10 06:33:35 minikube dockerd[1181]: time="2024-11-10T06:33:35.678111751Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=3eec5074f19cad81 traceID=2d8456aca29a66b74603860ed9700afa
Nov 10 06:33:35 minikube dockerd[1181]: time="2024-11-10T06:33:35.678247834Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=3eec5074f19cad81 traceID=2d8456aca29a66b74603860ed9700afa
Nov 10 06:33:35 minikube dockerd[1181]: time="2024-11-10T06:33:35.683440668Z" level=error msg="Handler for POST /v1.44/images/create returned error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=3eec5074f19cad81 traceID=2d8456aca29a66b74603860ed9700afa
Nov 10 06:35:43 minikube dockerd[1181]: time="2024-11-10T06:35:43.609502255Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=9ab3b1182a6d8b86 traceID=2b2891d9b1fe9a31dc13b4cb51ed6c80
Nov 10 06:35:43 minikube dockerd[1181]: time="2024-11-10T06:35:43.610174797Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=9ab3b1182a6d8b86 traceID=2b2891d9b1fe9a31dc13b4cb51ed6c80
Nov 10 06:35:43 minikube dockerd[1181]: time="2024-11-10T06:35:43.618546380Z" level=error msg="Handler for POST /v1.44/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=9ab3b1182a6d8b86 traceID=2b2891d9b1fe9a31dc13b4cb51ed6c80
Nov 10 06:38:23 minikube dockerd[1181]: time="2024-11-10T06:38:23.023596551Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=894469de18a70302 traceID=8778fcd9881e369536e548101b86550b
Nov 10 06:38:23 minikube dockerd[1181]: time="2024-11-10T06:38:23.024164468Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=894469de18a70302 traceID=8778fcd9881e369536e548101b86550b
Nov 10 06:38:23 minikube dockerd[1181]: time="2024-11-10T06:38:23.029065134Z" level=error msg="Handler for POST /v1.44/images/create returned error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=894469de18a70302 traceID=8778fcd9881e369536e548101b86550b
Nov 10 06:38:30 minikube dockerd[1181]: time="2024-11-10T06:38:30.648683138Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=0f3422611a9e380c traceID=9aa515a9191375a0576a2774213d8d7c
Nov 10 06:38:30 minikube dockerd[1181]: time="2024-11-10T06:38:30.649087430Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=0f3422611a9e380c traceID=9aa515a9191375a0576a2774213d8d7c
Nov 10 06:38:30 minikube dockerd[1181]: time="2024-11-10T06:38:30.654173763Z" level=error msg="Handler for POST /v1.44/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=0f3422611a9e380c traceID=9aa515a9191375a0576a2774213d8d7c
Nov 10 06:38:30 minikube dockerd[1181]: time="2024-11-10T06:38:30.931889680Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=0e40f59aa4001eb3 traceID=dfa051cfd78a03737a342e28ec5a0544
Nov 10 06:38:30 minikube dockerd[1181]: time="2024-11-10T06:38:30.932093888Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=0e40f59aa4001eb3 traceID=dfa051cfd78a03737a342e28ec5a0544
Nov 10 06:38:30 minikube dockerd[1181]: time="2024-11-10T06:38:30.937093513Z" level=error msg="Handler for POST /v1.44/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=0e40f59aa4001eb3 traceID=dfa051cfd78a03737a342e28ec5a0544
Nov 10 06:38:38 minikube dockerd[1181]: time="2024-11-10T06:38:38.638127752Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=9f4b85bdfee6dcc8 traceID=90b20a8dc3b2152ea182eb0ae0d097b1
Nov 10 06:38:38 minikube dockerd[1181]: time="2024-11-10T06:38:38.638293877Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=9f4b85bdfee6dcc8 traceID=90b20a8dc3b2152ea182eb0ae0d097b1
Nov 10 06:38:38 minikube dockerd[1181]: time="2024-11-10T06:38:38.643421169Z" level=error msg="Handler for POST /v1.44/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=9f4b85bdfee6dcc8 traceID=90b20a8dc3b2152ea182eb0ae0d097b1
Nov 10 06:38:45 minikube dockerd[1181]: time="2024-11-10T06:38:45.761774589Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=026e5ef305d6b911 traceID=07819a5e936236b0cb9a2cf69d25680c
Nov 10 06:38:45 minikube dockerd[1181]: time="2024-11-10T06:38:45.761896089Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=026e5ef305d6b911 traceID=07819a5e936236b0cb9a2cf69d25680c
Nov 10 06:38:45 minikube dockerd[1181]: time="2024-11-10T06:38:45.766900673Z" level=error msg="Handler for POST /v1.44/images/create returned error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" spanID=026e5ef305d6b911 traceID=07819a5e936236b0cb9a2cf69d25680c


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
a6443edd3c653       2437cf7621777       23 minutes ago      Running             coredns                   0                   07bf4b147371f       coredns-7db6d8ff4d-mjjqb
8aa0422c245fb       2437cf7621777       23 minutes ago      Running             coredns                   0                   2ce58ceed61bc       coredns-7db6d8ff4d-9n9n9
8f79504c621af       cb7eac0b42cc1       23 minutes ago      Running             kube-proxy                0                   e4df5ec302cbe       kube-proxy-5pxf7
9ccfed44a5121       ba04bb24b9575       23 minutes ago      Running             storage-provisioner       0                   36fea43ffe39b       storage-provisioner
e7ff1d690ed85       181f57fd3cdb7       23 minutes ago      Running             kube-apiserver            0                   b76eac65298fb       kube-apiserver-minikube
d1804cb741f2f       547adae34140b       23 minutes ago      Running             kube-scheduler            0                   828df8e713388       kube-scheduler-minikube
36be633795c3e       014faa467e297       23 minutes ago      Running             etcd                      0                   9617bb805775e       etcd-minikube
7da202a04bf08       68feac521c0f1       23 minutes ago      Running             kube-controller-manager   0                   57db73d4a5356       kube-controller-manager-minikube


==> coredns [8aa0422c245f] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = 591cf328cccc12bc490481273e738df59329c62c0b729d94e8b61db9961c2fa5f046dd37f1cf888b953814040d180f52594972691cd6ff41be96639138a43908
CoreDNS-1.11.1
linux/arm64, go1.20.7, ae2bbc2
[ERROR] plugin/errors: 2 2472401232900914243.3324218517717256931. HINFO: read udp 10.244.0.2:50345->192.168.65.254:53: i/o timeout


==> coredns [a6443edd3c65] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = 591cf328cccc12bc490481273e738df59329c62c0b729d94e8b61db9961c2fa5f046dd37f1cf888b953814040d180f52594972691cd6ff41be96639138a43908
CoreDNS-1.11.1
linux/arm64, go1.20.7, ae2bbc2
[ERROR] plugin/errors: 2 7430466534934802488.2744646000216232617. HINFO: read udp 10.244.0.4:45592->192.168.65.254:53: i/o timeout


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=5883c09216182566a63dff4c326a6fc9ed2982ff
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_11_10T09_17_06_0700
                    minikube.k8s.io/version=v1.33.1
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 10 Nov 2024 06:17:03 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 10 Nov 2024 06:40:35 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 10 Nov 2024 06:37:52 +0000   Sun, 10 Nov 2024 06:17:03 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 10 Nov 2024 06:37:52 +0000   Sun, 10 Nov 2024 06:17:03 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 10 Nov 2024 06:37:52 +0000   Sun, 10 Nov 2024 06:17:03 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 10 Nov 2024 06:37:52 +0000   Sun, 10 Nov 2024 06:17:03 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  61202244Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             8029248Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  61202244Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             8029248Ki
  pods:               110
System Info:
  Machine ID:                 6dd7801b2aab4e5c86a5a019c04598ee
  System UUID:                6dd7801b2aab4e5c86a5a019c04598ee
  Boot ID:                    880a03f8-6f0f-4644-bade-66a4ae0c46ee
  Kernel Version:             6.6.26-linuxkit
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               arm64
  Container Runtime Version:  docker://26.1.1
  Kubelet Version:            v1.30.0
  Kube-Proxy Version:         v1.30.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (15 in total)
  Namespace                   Name                                                 CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                                 ------------  ----------  ---------------  -------------  ---
  canary-demo                 prometheus-kube-prometheus-admission-create-mz7mg    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         15m
  ingress-nginx               ingress-nginx-admission-create-lgsxk                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         23m
  ingress-nginx               ingress-nginx-admission-patch-npww6                  0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         23m
  ingress-nginx               ingress-nginx-controller-768f948f8f-66wwr            100m (1%!)(MISSING)     0 (0%!)(MISSING)      90Mi (1%!)(MISSING)        0 (0%!)(MISSING)         23m
  kube-system                 coredns-7db6d8ff4d-9n9n9                             100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     23m
  kube-system                 coredns-7db6d8ff4d-mjjqb                             100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     23m
  kube-system                 etcd-minikube                                        100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         23m
  kube-system                 kube-apiserver-minikube                              250m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         23m
  kube-system                 kube-controller-manager-minikube                     200m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         23m
  kube-system                 kube-proxy-5pxf7                                     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         23m
  kube-system                 kube-scheduler-minikube                              100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         23m
  kube-system                 metrics-server-c59844bb4-r9vfs                       100m (1%!)(MISSING)     0 (0%!)(MISSING)      200Mi (2%!)(MISSING)       0 (0%!)(MISSING)         23m
  kube-system                 storage-provisioner                                  0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         23m
  kubernetes-dashboard        dashboard-metrics-scraper-b5fc48f67-42t9g            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         23m
  kubernetes-dashboard        kubernetes-dashboard-779776cb65-p8cm7                0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         23m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                1050m (13%!)(MISSING)  0 (0%!)(MISSING)
  memory             530Mi (6%!)(MISSING)   340Mi (4%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-32Mi     0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-64Ki     0 (0%!)(MISSING)       0 (0%!)(MISSING)
Events:
  Type    Reason                   Age   From             Message
  ----    ------                   ----  ----             -------
  Normal  Starting                 23m   kube-proxy       
  Normal  Starting                 23m   kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  23m   kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  23m   kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    23m   kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     23m   kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           23m   node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Nov10 05:17] cacheinfo: Unable to detect cache hierarchy for CPU 0
[  +0.170716] netlink: 'init': attribute type 4 has an invalid length.
[  +0.030059] fakeowner: loading out-of-tree module taints kernel.
[  +0.018444] setting FUSE negative_dentry_timeout to 3600 seconds
[  +0.000003] setting FUSE entry_timeout to 3600 seconds
[  +0.000001] setting FUSE attr_timeout to 3600 seconds
[  +0.000000] ignoring STATX_ATIME
[  +0.000000] returning ENOSYS from FUSE_FLUSH
[  +0.000001] overriding FOPEN_KEEP_CACHE
[  +7.705347] systemd[634]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set


==> etcd [36be633795c3] <==
{"level":"warn","ts":"2024-11-10T06:17:02.291247Z","caller":"embed/config.go:679","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-11-10T06:17:02.291343Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"warn","ts":"2024-11-10T06:17:02.291446Z","caller":"embed/config.go:679","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-11-10T06:17:02.291456Z","caller":"embed/etcd.go:127","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-11-10T06:17:02.291488Z","caller":"embed/etcd.go:494","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-11-10T06:17:02.291951Z","caller":"embed/etcd.go:135","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-11-10T06:17:02.292Z","caller":"embed/etcd.go:308","msg":"starting an etcd server","etcd-version":"3.5.12","git-sha":"e7b3bb6cc","go-version":"go1.20.13","go-os":"linux","go-arch":"arm64","max-cpu-set":8,"max-cpu-available":8,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://192.168.49.2:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-11-10T06:17:02.293283Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"1.16825ms"}
{"level":"info","ts":"2024-11-10T06:17:02.295657Z","caller":"etcdserver/raft.go:495","msg":"starting local member","local-member-id":"aec36adc501070cc","cluster-id":"fa54960ea34d58be"}
{"level":"info","ts":"2024-11-10T06:17:02.295696Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2024-11-10T06:17:02.295723Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 0"}
{"level":"info","ts":"2024-11-10T06:17:02.295729Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2024-11-10T06:17:02.295732Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 1"}
{"level":"info","ts":"2024-11-10T06:17:02.29575Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"warn","ts":"2024-11-10T06:17:02.297318Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-11-10T06:17:02.297694Z","caller":"mvcc/kvstore.go:407","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2024-11-10T06:17:02.298056Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-11-10T06:17:02.298818Z","caller":"etcdserver/server.go:860","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.12","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-11-10T06:17:02.298965Z","caller":"etcdserver/server.go:744","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-11-10T06:17:02.299165Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-11-10T06:17:02.299213Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-11-10T06:17:02.299226Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-11-10T06:17:02.300578Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-11-10T06:17:02.300691Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-11-10T06:17:02.302243Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-11-10T06:17:02.302322Z","caller":"embed/etcd.go:277","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-11-10T06:17:02.302333Z","caller":"embed/etcd.go:857","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-11-10T06:17:02.302379Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-11-10T06:17:02.302384Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-11-10T06:17:02.696801Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2024-11-10T06:17:02.696827Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2024-11-10T06:17:02.696842Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2024-11-10T06:17:02.696859Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2024-11-10T06:17:02.696867Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2024-11-10T06:17:02.696875Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2024-11-10T06:17:02.696879Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2024-11-10T06:17:02.697206Z","caller":"etcdserver/server.go:2578","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2024-11-10T06:17:02.697531Z","caller":"etcdserver/server.go:2068","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-11-10T06:17:02.697563Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-11-10T06:17:02.697619Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2024-11-10T06:17:02.697677Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-11-10T06:17:02.697686Z","caller":"etcdserver/server.go:2602","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2024-11-10T06:17:02.697553Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-11-10T06:17:02.697698Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-11-10T06:17:02.697723Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-11-10T06:17:02.698487Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-11-10T06:17:02.698497Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-11-10T06:27:02.703644Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":932}
{"level":"info","ts":"2024-11-10T06:27:02.712894Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":932,"took":"8.269625ms","hash":899369478,"current-db-size-bytes":13459456,"current-db-size":"14 MB","current-db-size-in-use-bytes":12308480,"current-db-size-in-use":"12 MB"}
{"level":"info","ts":"2024-11-10T06:27:02.712972Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":899369478,"revision":932,"compact-revision":-1}
{"level":"info","ts":"2024-11-10T06:32:02.71608Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1275}
{"level":"info","ts":"2024-11-10T06:32:02.74132Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":1275,"took":"23.821917ms","hash":4249672405,"current-db-size-bytes":16039936,"current-db-size":"16 MB","current-db-size-in-use-bytes":13434880,"current-db-size-in-use":"13 MB"}
{"level":"info","ts":"2024-11-10T06:32:02.741364Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4249672405,"revision":1275,"compact-revision":932}
{"level":"info","ts":"2024-11-10T06:37:02.712905Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1542}
{"level":"info","ts":"2024-11-10T06:37:02.725692Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":1542,"took":"11.831209ms","hash":153868181,"current-db-size-bytes":16039936,"current-db-size":"16 MB","current-db-size-in-use-bytes":4964352,"current-db-size-in-use":"5.0 MB"}
{"level":"info","ts":"2024-11-10T06:37:02.725795Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":153868181,"revision":1542,"compact-revision":1275}


==> kernel <==
 06:40:43 up  1:23,  0 users,  load average: 3.16, 3.39, 3.31
Linux minikube 6.6.26-linuxkit #1 SMP Sat Apr 27 04:13:19 UTC 2024 aarch64 aarch64 aarch64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [e7ff1d690ed8] <==
I1110 06:28:04.664516       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W1110 06:28:04.667735       1 handler_proxy.go:93] no RequestInfo found in the context
E1110 06:28:04.668029       1 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I1110 06:28:04.668111       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W1110 06:30:04.666084       1 handler_proxy.go:93] no RequestInfo found in the context
E1110 06:30:04.666392       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I1110 06:30:04.666456       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W1110 06:30:04.670175       1 handler_proxy.go:93] no RequestInfo found in the context
E1110 06:30:04.670474       1 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I1110 06:30:04.670549       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W1110 06:32:03.664527       1 handler_proxy.go:93] no RequestInfo found in the context
E1110 06:32:03.664851       1 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
W1110 06:32:04.665332       1 handler_proxy.go:93] no RequestInfo found in the context
E1110 06:32:04.665528       1 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I1110 06:32:04.665548       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W1110 06:32:04.665332       1 handler_proxy.go:93] no RequestInfo found in the context
E1110 06:32:04.665613       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I1110 06:32:04.667229       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W1110 06:33:04.666643       1 handler_proxy.go:93] no RequestInfo found in the context
E1110 06:33:04.666835       1 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I1110 06:33:04.666878       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W1110 06:33:04.668204       1 handler_proxy.go:93] no RequestInfo found in the context
E1110 06:33:04.668271       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I1110 06:33:04.668288       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W1110 06:35:04.655550       1 handler_proxy.go:93] no RequestInfo found in the context
E1110 06:35:04.655752       1 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I1110 06:35:04.655784       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W1110 06:35:04.657121       1 handler_proxy.go:93] no RequestInfo found in the context
E1110 06:35:04.657160       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I1110 06:35:04.657168       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W1110 06:37:03.668437       1 handler_proxy.go:93] no RequestInfo found in the context
E1110 06:37:03.668812       1 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
W1110 06:37:04.669543       1 handler_proxy.go:93] no RequestInfo found in the context
W1110 06:37:04.669618       1 handler_proxy.go:93] no RequestInfo found in the context
E1110 06:37:04.669652       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I1110 06:37:04.669676       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E1110 06:37:04.669987       1 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I1110 06:37:04.671298       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W1110 06:38:04.669935       1 handler_proxy.go:93] no RequestInfo found in the context
W1110 06:38:04.671744       1 handler_proxy.go:93] no RequestInfo found in the context
E1110 06:38:04.671886       1 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I1110 06:38:04.671904       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E1110 06:38:04.672302       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I1110 06:38:04.673174       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W1110 06:40:04.672279       1 handler_proxy.go:93] no RequestInfo found in the context
W1110 06:40:04.672534       1 handler_proxy.go:93] no RequestInfo found in the context
E1110 06:40:04.672702       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I1110 06:40:04.672783       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E1110 06:40:04.674762       1 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I1110 06:40:04.674791       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.


==> kube-controller-manager [7da202a04bf0] <==
E1110 06:31:19.629521       1 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I1110 06:31:19.802686       1 garbagecollector.go:826] "failed to discover some groups" logger="garbage-collector-controller" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
E1110 06:31:49.642615       1 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I1110 06:31:49.811576       1 garbagecollector.go:826] "failed to discover some groups" logger="garbage-collector-controller" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
E1110 06:32:19.658699       1 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I1110 06:32:19.821025       1 garbagecollector.go:826] "failed to discover some groups" logger="garbage-collector-controller" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
E1110 06:32:49.669253       1 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I1110 06:32:49.834811       1 garbagecollector.go:826] "failed to discover some groups" logger="garbage-collector-controller" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
E1110 06:33:19.683197       1 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I1110 06:33:19.842210       1 garbagecollector.go:826] "failed to discover some groups" logger="garbage-collector-controller" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
I1110 06:33:30.383678       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-779776cb65" duration="330.666¬µs"
I1110 06:33:36.382577       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-c59844bb4" duration="245.167¬µs"
I1110 06:33:39.387247       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I1110 06:33:43.386318       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-779776cb65" duration="92.125¬µs"
I1110 06:33:43.397528       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I1110 06:33:46.384501       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" duration="218.083¬µs"
I1110 06:33:47.388985       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-c59844bb4" duration="65.167¬µs"
E1110 06:33:49.693556       1 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I1110 06:33:49.847436       1 garbagecollector.go:826] "failed to discover some groups" logger="garbage-collector-controller" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
I1110 06:33:53.386042       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I1110 06:33:55.385172       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I1110 06:34:01.381368       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" duration="183.75¬µs"
E1110 06:34:19.707522       1 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I1110 06:34:19.880656       1 garbagecollector.go:826] "failed to discover some groups" logger="garbage-collector-controller" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
E1110 06:34:49.709138       1 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I1110 06:34:49.882554       1 garbagecollector.go:826] "failed to discover some groups" logger="garbage-collector-controller" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
E1110 06:35:19.718943       1 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I1110 06:35:19.887130       1 garbagecollector.go:826] "failed to discover some groups" logger="garbage-collector-controller" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
E1110 06:35:49.730781       1 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I1110 06:35:49.897750       1 garbagecollector.go:826] "failed to discover some groups" logger="garbage-collector-controller" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
I1110 06:35:55.368230       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="canary-demo/prometheus-kube-prometheus-admission-create"
I1110 06:36:08.371140       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="canary-demo/prometheus-kube-prometheus-admission-create"
E1110 06:36:19.740651       1 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I1110 06:36:19.931824       1 garbagecollector.go:826] "failed to discover some groups" logger="garbage-collector-controller" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
E1110 06:36:49.758447       1 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I1110 06:36:49.942428       1 garbagecollector.go:826] "failed to discover some groups" logger="garbage-collector-controller" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
E1110 06:37:19.767301       1 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I1110 06:37:19.950045       1 garbagecollector.go:826] "failed to discover some groups" logger="garbage-collector-controller" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
E1110 06:37:49.773718       1 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I1110 06:37:49.964850       1 garbagecollector.go:826] "failed to discover some groups" logger="garbage-collector-controller" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
E1110 06:38:19.781581       1 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I1110 06:38:19.981684       1 garbagecollector.go:826] "failed to discover some groups" logger="garbage-collector-controller" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
I1110 06:38:35.379835       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-779776cb65" duration="827.708¬µs"
I1110 06:38:44.374862       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-c59844bb4" duration="199.042¬µs"
I1110 06:38:46.378511       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I1110 06:38:47.377455       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-779776cb65" duration="200.708¬µs"
I1110 06:38:49.374983       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
E1110 06:38:49.787089       1 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I1110 06:38:49.999534       1 garbagecollector.go:826] "failed to discover some groups" logger="garbage-collector-controller" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
I1110 06:38:57.374765       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" duration="110.917¬µs"
I1110 06:38:59.384824       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-c59844bb4" duration="125.333¬µs"
I1110 06:39:01.373740       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch"
I1110 06:39:04.373874       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create"
I1110 06:39:10.360761       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67" duration="136.042¬µs"
E1110 06:39:19.793005       1 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I1110 06:39:20.012581       1 garbagecollector.go:826] "failed to discover some groups" logger="garbage-collector-controller" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
E1110 06:39:49.801272       1 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I1110 06:39:50.030223       1 garbagecollector.go:826] "failed to discover some groups" logger="garbage-collector-controller" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
E1110 06:40:19.806027       1 resource_quota_controller.go:440] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
I1110 06:40:20.043993       1 garbagecollector.go:826] "failed to discover some groups" logger="garbage-collector-controller" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"


==> kube-proxy [8f79504c621a] <==
I1110 06:17:20.394056       1 server_linux.go:69] "Using iptables proxy"
I1110 06:17:20.479628       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I1110 06:17:20.583203       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1110 06:17:20.583238       1 server_linux.go:165] "Using iptables Proxier"
I1110 06:17:20.585152       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I1110 06:17:20.585332       1 server_linux.go:528] "Defaulting to no-op detect-local"
I1110 06:17:20.585367       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I1110 06:17:20.585520       1 server.go:872] "Version info" version="v1.30.0"
I1110 06:17:20.585531       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1110 06:17:20.588850       1 config.go:192] "Starting service config controller"
I1110 06:17:20.588880       1 shared_informer.go:313] Waiting for caches to sync for service config
I1110 06:17:20.588882       1 config.go:319] "Starting node config controller"
I1110 06:17:20.589029       1 shared_informer.go:313] Waiting for caches to sync for node config
I1110 06:17:20.589002       1 config.go:101] "Starting endpoint slice config controller"
I1110 06:17:20.589042       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1110 06:17:20.689434       1 shared_informer.go:320] Caches are synced for service config
I1110 06:17:20.689442       1 shared_informer.go:320] Caches are synced for node config
I1110 06:17:20.689445       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-scheduler [d1804cb741f2] <==
I1110 06:17:02.491107       1 serving.go:380] Generated self-signed cert in-memory
W1110 06:17:03.574844       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1110 06:17:03.574915       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1110 06:17:03.574928       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W1110 06:17:03.574944       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1110 06:17:03.585091       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I1110 06:17:03.585105       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1110 06:17:03.586392       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1110 06:17:03.586416       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1110 06:17:03.586461       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1110 06:17:03.586581       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
W1110 06:17:03.587466       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1110 06:17:03.587509       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W1110 06:17:03.587529       1 reflector.go:547] runtime/asm_arm64.s:1222: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W1110 06:17:03.587537       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1110 06:17:03.587549       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1110 06:17:03.587551       1 reflector.go:150] runtime/asm_arm64.s:1222: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W1110 06:17:03.587564       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1110 06:17:03.587568       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W1110 06:17:03.587572       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1110 06:17:03.587581       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W1110 06:17:03.587600       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W1110 06:17:03.587608       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1110 06:17:03.587612       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1110 06:17:03.587615       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1110 06:17:03.587605       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1110 06:17:03.587634       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W1110 06:17:03.587642       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1110 06:17:03.587647       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1110 06:17:03.587660       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1110 06:17:03.587664       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W1110 06:17:03.587671       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1110 06:17:03.587676       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W1110 06:17:03.587704       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1110 06:17:03.587713       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1110 06:17:03.587715       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1110 06:17:03.587732       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W1110 06:17:03.587884       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1110 06:17:03.587899       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W1110 06:17:03.587923       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1110 06:17:03.587931       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W1110 06:17:04.435415       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1110 06:17:04.435445       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1110 06:17:04.488517       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1110 06:17:04.488535       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W1110 06:17:04.508812       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1110 06:17:04.508829       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
I1110 06:17:04.786649       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Nov 10 06:38:45 minikube kubelet[2269]: E1110 06:38:45.768641    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"dashboard-metrics-scraper\" with ErrImagePull: \"Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67-42t9g" podUID="0a552b55-7a68-4af3-b953-c4ad582411cb"
Nov 10 06:38:46 minikube kubelet[2269]: E1110 06:38:46.354413    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.1@sha256:36d05b4077fb8e3d13663702fa337f124675ba8667cbd949c03a8e8ea6fa4366\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-npww6" podUID="8c1f2963-9bd6-413d-a1f0-b2f2bbd05ae0"
Nov 10 06:38:47 minikube kubelet[2269]: E1110 06:38:47.362520    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6\\\"\"" pod="canary-demo/prometheus-kube-prometheus-admission-create-mz7mg" podUID="e9d55624-03ac-4e70-95fb-436f4f2003f0"
Nov 10 06:38:47 minikube kubelet[2269]: E1110 06:38:47.363507    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93\\\"\"" pod="kubernetes-dashboard/kubernetes-dashboard-779776cb65-p8cm7" podUID="259622b7-3dbe-4ad4-8168-2d9fe98227d8"
Nov 10 06:38:49 minikube kubelet[2269]: E1110 06:38:49.358640    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.1@sha256:36d05b4077fb8e3d13663702fa337f124675ba8667cbd949c03a8e8ea6fa4366\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-lgsxk" podUID="25c3db04-7ba2-4630-becd-0cf79ea5b374"
Nov 10 06:38:57 minikube kubelet[2269]: E1110 06:38:57.360760    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"dashboard-metrics-scraper\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c\\\"\"" pod="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67-42t9g" podUID="0a552b55-7a68-4af3-b953-c4ad582411cb"
Nov 10 06:38:59 minikube kubelet[2269]: E1110 06:38:59.365499    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6\\\"\"" pod="canary-demo/prometheus-kube-prometheus-admission-create-mz7mg" podUID="e9d55624-03ac-4e70-95fb-436f4f2003f0"
Nov 10 06:38:59 minikube kubelet[2269]: E1110 06:38:59.366587    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/metrics-server/metrics-server:v0.7.1@sha256:db3800085a0957083930c3932b17580eec652cfb6156a05c0f79c7543e80d17a\\\"\"" pod="kube-system/metrics-server-c59844bb4-r9vfs" podUID="871235b1-82ca-4ac7-bd65-2bd105e6ea6b"
Nov 10 06:39:00 minikube kubelet[2269]: E1110 06:39:00.354142    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93\\\"\"" pod="kubernetes-dashboard/kubernetes-dashboard-779776cb65-p8cm7" podUID="259622b7-3dbe-4ad4-8168-2d9fe98227d8"
Nov 10 06:39:01 minikube kubelet[2269]: E1110 06:39:01.359725    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.1@sha256:36d05b4077fb8e3d13663702fa337f124675ba8667cbd949c03a8e8ea6fa4366\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-npww6" podUID="8c1f2963-9bd6-413d-a1f0-b2f2bbd05ae0"
Nov 10 06:39:04 minikube kubelet[2269]: E1110 06:39:04.358477    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.1@sha256:36d05b4077fb8e3d13663702fa337f124675ba8667cbd949c03a8e8ea6fa4366\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-lgsxk" podUID="25c3db04-7ba2-4630-becd-0cf79ea5b374"
Nov 10 06:39:10 minikube kubelet[2269]: E1110 06:39:10.352965    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"dashboard-metrics-scraper\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c\\\"\"" pod="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67-42t9g" podUID="0a552b55-7a68-4af3-b953-c4ad582411cb"
Nov 10 06:39:12 minikube kubelet[2269]: E1110 06:39:12.356994    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93\\\"\"" pod="kubernetes-dashboard/kubernetes-dashboard-779776cb65-p8cm7" podUID="259622b7-3dbe-4ad4-8168-2d9fe98227d8"
Nov 10 06:39:12 minikube kubelet[2269]: E1110 06:39:12.357119    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/metrics-server/metrics-server:v0.7.1@sha256:db3800085a0957083930c3932b17580eec652cfb6156a05c0f79c7543e80d17a\\\"\"" pod="kube-system/metrics-server-c59844bb4-r9vfs" podUID="871235b1-82ca-4ac7-bd65-2bd105e6ea6b"
Nov 10 06:39:13 minikube kubelet[2269]: E1110 06:39:13.357978    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6\\\"\"" pod="canary-demo/prometheus-kube-prometheus-admission-create-mz7mg" podUID="e9d55624-03ac-4e70-95fb-436f4f2003f0"
Nov 10 06:39:15 minikube kubelet[2269]: E1110 06:39:15.359272    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.1@sha256:36d05b4077fb8e3d13663702fa337f124675ba8667cbd949c03a8e8ea6fa4366\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-lgsxk" podUID="25c3db04-7ba2-4630-becd-0cf79ea5b374"
Nov 10 06:39:16 minikube kubelet[2269]: E1110 06:39:16.353278    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.1@sha256:36d05b4077fb8e3d13663702fa337f124675ba8667cbd949c03a8e8ea6fa4366\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-npww6" podUID="8c1f2963-9bd6-413d-a1f0-b2f2bbd05ae0"
Nov 10 06:39:23 minikube kubelet[2269]: E1110 06:39:23.358586    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93\\\"\"" pod="kubernetes-dashboard/kubernetes-dashboard-779776cb65-p8cm7" podUID="259622b7-3dbe-4ad4-8168-2d9fe98227d8"
Nov 10 06:39:23 minikube kubelet[2269]: E1110 06:39:23.360020    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/metrics-server/metrics-server:v0.7.1@sha256:db3800085a0957083930c3932b17580eec652cfb6156a05c0f79c7543e80d17a\\\"\"" pod="kube-system/metrics-server-c59844bb4-r9vfs" podUID="871235b1-82ca-4ac7-bd65-2bd105e6ea6b"
Nov 10 06:39:23 minikube kubelet[2269]: E1110 06:39:23.360739    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"dashboard-metrics-scraper\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c\\\"\"" pod="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67-42t9g" podUID="0a552b55-7a68-4af3-b953-c4ad582411cb"
Nov 10 06:39:26 minikube kubelet[2269]: E1110 06:39:26.352438    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6\\\"\"" pod="canary-demo/prometheus-kube-prometheus-admission-create-mz7mg" podUID="e9d55624-03ac-4e70-95fb-436f4f2003f0"
Nov 10 06:39:26 minikube kubelet[2269]: E1110 06:39:26.352479    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.1@sha256:36d05b4077fb8e3d13663702fa337f124675ba8667cbd949c03a8e8ea6fa4366\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-lgsxk" podUID="25c3db04-7ba2-4630-becd-0cf79ea5b374"
Nov 10 06:39:27 minikube kubelet[2269]: E1110 06:39:27.357703    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.1@sha256:36d05b4077fb8e3d13663702fa337f124675ba8667cbd949c03a8e8ea6fa4366\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-npww6" podUID="8c1f2963-9bd6-413d-a1f0-b2f2bbd05ae0"
Nov 10 06:39:34 minikube kubelet[2269]: E1110 06:39:34.361580    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"dashboard-metrics-scraper\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c\\\"\"" pod="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67-42t9g" podUID="0a552b55-7a68-4af3-b953-c4ad582411cb"
Nov 10 06:39:35 minikube kubelet[2269]: E1110 06:39:35.361759    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/metrics-server/metrics-server:v0.7.1@sha256:db3800085a0957083930c3932b17580eec652cfb6156a05c0f79c7543e80d17a\\\"\"" pod="kube-system/metrics-server-c59844bb4-r9vfs" podUID="871235b1-82ca-4ac7-bd65-2bd105e6ea6b"
Nov 10 06:39:37 minikube kubelet[2269]: E1110 06:39:37.368946    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6\\\"\"" pod="canary-demo/prometheus-kube-prometheus-admission-create-mz7mg" podUID="e9d55624-03ac-4e70-95fb-436f4f2003f0"
Nov 10 06:39:38 minikube kubelet[2269]: E1110 06:39:38.357769    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93\\\"\"" pod="kubernetes-dashboard/kubernetes-dashboard-779776cb65-p8cm7" podUID="259622b7-3dbe-4ad4-8168-2d9fe98227d8"
Nov 10 06:39:40 minikube kubelet[2269]: E1110 06:39:40.359650    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.1@sha256:36d05b4077fb8e3d13663702fa337f124675ba8667cbd949c03a8e8ea6fa4366\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-lgsxk" podUID="25c3db04-7ba2-4630-becd-0cf79ea5b374"
Nov 10 06:39:40 minikube kubelet[2269]: E1110 06:39:40.359773    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.1@sha256:36d05b4077fb8e3d13663702fa337f124675ba8667cbd949c03a8e8ea6fa4366\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-npww6" podUID="8c1f2963-9bd6-413d-a1f0-b2f2bbd05ae0"
Nov 10 06:39:46 minikube kubelet[2269]: E1110 06:39:46.353734    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/metrics-server/metrics-server:v0.7.1@sha256:db3800085a0957083930c3932b17580eec652cfb6156a05c0f79c7543e80d17a\\\"\"" pod="kube-system/metrics-server-c59844bb4-r9vfs" podUID="871235b1-82ca-4ac7-bd65-2bd105e6ea6b"
Nov 10 06:39:47 minikube kubelet[2269]: E1110 06:39:47.357160    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"dashboard-metrics-scraper\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c\\\"\"" pod="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67-42t9g" podUID="0a552b55-7a68-4af3-b953-c4ad582411cb"
Nov 10 06:39:48 minikube kubelet[2269]: E1110 06:39:48.355101    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="ingress-nginx/ingress-nginx-controller-768f948f8f-66wwr" podUID="7a233c62-0fbb-4cf8-add2-0fee7c3e90a8"
Nov 10 06:39:49 minikube kubelet[2269]: E1110 06:39:49.063790    2269 secret.go:194] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Nov 10 06:39:49 minikube kubelet[2269]: E1110 06:39:49.063945    2269 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/7a233c62-0fbb-4cf8-add2-0fee7c3e90a8-webhook-cert podName:7a233c62-0fbb-4cf8-add2-0fee7c3e90a8 nodeName:}" failed. No retries permitted until 2024-11-10 06:41:51.063894507 +0000 UTC m=+1485.754210623 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/7a233c62-0fbb-4cf8-add2-0fee7c3e90a8-webhook-cert") pod "ingress-nginx-controller-768f948f8f-66wwr" (UID: "7a233c62-0fbb-4cf8-add2-0fee7c3e90a8") : secret "ingress-nginx-admission" not found
Nov 10 06:39:49 minikube kubelet[2269]: E1110 06:39:49.358000    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6\\\"\"" pod="canary-demo/prometheus-kube-prometheus-admission-create-mz7mg" podUID="e9d55624-03ac-4e70-95fb-436f4f2003f0"
Nov 10 06:39:53 minikube kubelet[2269]: E1110 06:39:53.358985    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93\\\"\"" pod="kubernetes-dashboard/kubernetes-dashboard-779776cb65-p8cm7" podUID="259622b7-3dbe-4ad4-8168-2d9fe98227d8"
Nov 10 06:39:53 minikube kubelet[2269]: E1110 06:39:53.359154    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.1@sha256:36d05b4077fb8e3d13663702fa337f124675ba8667cbd949c03a8e8ea6fa4366\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-lgsxk" podUID="25c3db04-7ba2-4630-becd-0cf79ea5b374"
Nov 10 06:39:55 minikube kubelet[2269]: E1110 06:39:55.360132    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.1@sha256:36d05b4077fb8e3d13663702fa337f124675ba8667cbd949c03a8e8ea6fa4366\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-npww6" podUID="8c1f2963-9bd6-413d-a1f0-b2f2bbd05ae0"
Nov 10 06:39:58 minikube kubelet[2269]: E1110 06:39:58.357161    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"dashboard-metrics-scraper\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c\\\"\"" pod="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67-42t9g" podUID="0a552b55-7a68-4af3-b953-c4ad582411cb"
Nov 10 06:39:58 minikube kubelet[2269]: E1110 06:39:58.357292    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/metrics-server/metrics-server:v0.7.1@sha256:db3800085a0957083930c3932b17580eec652cfb6156a05c0f79c7543e80d17a\\\"\"" pod="kube-system/metrics-server-c59844bb4-r9vfs" podUID="871235b1-82ca-4ac7-bd65-2bd105e6ea6b"
Nov 10 06:40:02 minikube kubelet[2269]: E1110 06:40:02.358643    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6\\\"\"" pod="canary-demo/prometheus-kube-prometheus-admission-create-mz7mg" podUID="e9d55624-03ac-4e70-95fb-436f4f2003f0"
Nov 10 06:40:04 minikube kubelet[2269]: E1110 06:40:04.357980    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.1@sha256:36d05b4077fb8e3d13663702fa337f124675ba8667cbd949c03a8e8ea6fa4366\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-lgsxk" podUID="25c3db04-7ba2-4630-becd-0cf79ea5b374"
Nov 10 06:40:07 minikube kubelet[2269]: E1110 06:40:07.355406    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.1@sha256:36d05b4077fb8e3d13663702fa337f124675ba8667cbd949c03a8e8ea6fa4366\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-npww6" podUID="8c1f2963-9bd6-413d-a1f0-b2f2bbd05ae0"
Nov 10 06:40:07 minikube kubelet[2269]: E1110 06:40:07.355559    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93\\\"\"" pod="kubernetes-dashboard/kubernetes-dashboard-779776cb65-p8cm7" podUID="259622b7-3dbe-4ad4-8168-2d9fe98227d8"
Nov 10 06:40:10 minikube kubelet[2269]: E1110 06:40:10.359520    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/metrics-server/metrics-server:v0.7.1@sha256:db3800085a0957083930c3932b17580eec652cfb6156a05c0f79c7543e80d17a\\\"\"" pod="kube-system/metrics-server-c59844bb4-r9vfs" podUID="871235b1-82ca-4ac7-bd65-2bd105e6ea6b"
Nov 10 06:40:13 minikube kubelet[2269]: E1110 06:40:13.355961    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"dashboard-metrics-scraper\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c\\\"\"" pod="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67-42t9g" podUID="0a552b55-7a68-4af3-b953-c4ad582411cb"
Nov 10 06:40:13 minikube kubelet[2269]: E1110 06:40:13.356092    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6\\\"\"" pod="canary-demo/prometheus-kube-prometheus-admission-create-mz7mg" podUID="e9d55624-03ac-4e70-95fb-436f4f2003f0"
Nov 10 06:40:15 minikube kubelet[2269]: E1110 06:40:15.358621    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.1@sha256:36d05b4077fb8e3d13663702fa337f124675ba8667cbd949c03a8e8ea6fa4366\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-lgsxk" podUID="25c3db04-7ba2-4630-becd-0cf79ea5b374"
Nov 10 06:40:21 minikube kubelet[2269]: E1110 06:40:21.357331    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93\\\"\"" pod="kubernetes-dashboard/kubernetes-dashboard-779776cb65-p8cm7" podUID="259622b7-3dbe-4ad4-8168-2d9fe98227d8"
Nov 10 06:40:22 minikube kubelet[2269]: E1110 06:40:22.357816    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.1@sha256:36d05b4077fb8e3d13663702fa337f124675ba8667cbd949c03a8e8ea6fa4366\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-npww6" podUID="8c1f2963-9bd6-413d-a1f0-b2f2bbd05ae0"
Nov 10 06:40:23 minikube kubelet[2269]: E1110 06:40:23.362438    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/metrics-server/metrics-server:v0.7.1@sha256:db3800085a0957083930c3932b17580eec652cfb6156a05c0f79c7543e80d17a\\\"\"" pod="kube-system/metrics-server-c59844bb4-r9vfs" podUID="871235b1-82ca-4ac7-bd65-2bd105e6ea6b"
Nov 10 06:40:24 minikube kubelet[2269]: E1110 06:40:24.354400    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"dashboard-metrics-scraper\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c\\\"\"" pod="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67-42t9g" podUID="0a552b55-7a68-4af3-b953-c4ad582411cb"
Nov 10 06:40:24 minikube kubelet[2269]: E1110 06:40:24.354400    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6\\\"\"" pod="canary-demo/prometheus-kube-prometheus-admission-create-mz7mg" podUID="e9d55624-03ac-4e70-95fb-436f4f2003f0"
Nov 10 06:40:29 minikube kubelet[2269]: E1110 06:40:29.355664    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.1@sha256:36d05b4077fb8e3d13663702fa337f124675ba8667cbd949c03a8e8ea6fa4366\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-lgsxk" podUID="25c3db04-7ba2-4630-becd-0cf79ea5b374"
Nov 10 06:40:33 minikube kubelet[2269]: E1110 06:40:33.359491    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.1@sha256:36d05b4077fb8e3d13663702fa337f124675ba8667cbd949c03a8e8ea6fa4366\\\"\"" pod="ingress-nginx/ingress-nginx-admission-patch-npww6" podUID="8c1f2963-9bd6-413d-a1f0-b2f2bbd05ae0"
Nov 10 06:40:33 minikube kubelet[2269]: E1110 06:40:33.359488    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93\\\"\"" pod="kubernetes-dashboard/kubernetes-dashboard-779776cb65-p8cm7" podUID="259622b7-3dbe-4ad4-8168-2d9fe98227d8"
Nov 10 06:40:35 minikube kubelet[2269]: E1110 06:40:35.359061    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"dashboard-metrics-scraper\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c\\\"\"" pod="kubernetes-dashboard/dashboard-metrics-scraper-b5fc48f67-42t9g" podUID="0a552b55-7a68-4af3-b953-c4ad582411cb"
Nov 10 06:40:37 minikube kubelet[2269]: E1110 06:40:37.356008    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/metrics-server/metrics-server:v0.7.1@sha256:db3800085a0957083930c3932b17580eec652cfb6156a05c0f79c7543e80d17a\\\"\"" pod="kube-system/metrics-server-c59844bb4-r9vfs" podUID="871235b1-82ca-4ac7-bd65-2bd105e6ea6b"
Nov 10 06:40:38 minikube kubelet[2269]: E1110 06:40:38.357641    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6\\\"\"" pod="canary-demo/prometheus-kube-prometheus-admission-create-mz7mg" podUID="e9d55624-03ac-4e70-95fb-436f4f2003f0"
Nov 10 06:40:41 minikube kubelet[2269]: E1110 06:40:41.356551    2269 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.1@sha256:36d05b4077fb8e3d13663702fa337f124675ba8667cbd949c03a8e8ea6fa4366\\\"\"" pod="ingress-nginx/ingress-nginx-admission-create-lgsxk" podUID="25c3db04-7ba2-4630-becd-0cf79ea5b374"


==> storage-provisioner [9ccfed44a512] <==
I1110 06:17:20.106526       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1110 06:17:26.128124       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1110 06:17:26.128242       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1110 06:17:26.138666       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1110 06:17:26.138797       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"64455ab1-fbbd-4e34-9e34-4621ef24a308", APIVersion:"v1", ResourceVersion:"567", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_5d917e2c-4959-4a45-b5e2-52638c79c9f4 became leader
I1110 06:17:26.138875       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_5d917e2c-4959-4a45-b5e2-52638c79c9f4!
I1110 06:17:26.239641       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_5d917e2c-4959-4a45-b5e2-52638c79c9f4!

